% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/source_subsel.R
\name{pp_loss}
\alias{pp_loss}
\title{Compute the predictive and empirical cross-validated squared error loss}
\usage{
pp_loss(
  post_y_pred,
  post_lpd,
  XX,
  yy,
  indicators,
  post_y_hat = NULL,
  K = 10,
  sir_frac = 0.5
)
}
\arguments{
\item{post_y_pred}{\code{S x n} matrix of posterior predictive
at the given \code{XX} covariate values}

\item{post_lpd}{\code{S} evaluations of the log-likelihood computed
at each posterior draw of the parameters}

\item{XX}{\code{n x p} matrix of covariates at which to evaluate}

\item{yy}{\code{n}-dimensional vector of response variables}

\item{indicators}{\code{L x p} matrix of inclusion indicators
where each row denotes a candidate subset}

\item{post_y_hat}{\code{S x n} matrix of posterior fitted values
at the given \code{XX} covariate values}

\item{K}{number of cross-validation folds}

\item{sir_frac}{fraction of the posterior samples to use for SIR}
}
\value{
a list with two elements: \code{pred_loss} and \code{emp_loss}
for the predictive and empirical loss, respectively, for each subset.
}
\description{
Use posterior predictive draws and a sampling-importance reweighting (SIR)
algorithm to approximate the cross-validated predictive squared error loss.
The empirical squared error loss (i.e., the usual quantity in cross-validation)
is also returned. The values are computed relative to the "best"
subset according to minimum empirical squared error loss.
Specifically, these quantities are computed for a collection of
linear models that are fit to the Bayesian model output, where
each linear model features a different subset of predictors.
}
\details{
The quantity \code{post_y_hat} is the conditional expectation of the
response for each covariate value (columns) and using the parameters sampled
from the posterior (rows). For Bayesian linear regression, this term is
\code{Xbeta}. If unspecified, the algorithm will instead use \code{post_y_pred},
which is still correct but has lower Monte Carlo efficiency.
}
