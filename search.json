[{"path":"https://drkowal.github.io/BayesSubsets/articles/BayesSubsets.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Introduction to BayesSubsets","text":"Subset selection valuable tool interpretable learning, scientific discovery, data compression. Given outcome \\(y\\) \\((n \\times p)\\) covariates \\(X\\), general goal find subset covariates (.e., columns \\(X\\)) “best” predict \\(y\\), estimate corresponding regression coefficients. recent years, subset selection declined popularity several reasons: Selection instability: common obtain completely different “best” subsets minor perturbations data; Lack regularization: “best” subset identified, coefficients estimated using ordinary least squares (OLS)—inadvisable subset moderate/large covariates correlated, since can induce variance-inflation; Computational bottlenecks: exhaustive search must consider \\(2^p\\) subsets, becomes prohibitive \\(p > 30\\). result, frequentists pursued penalized regression (e.g., lasso), also commonly select variables based p-values (e.g., p-values \\(< 0.05\\))—although typically referred “selection” explicitly. Similarly, Bayesian methods predominantly consider marginal selection, example using posterior inclusion probabilities (e.g., spike--slab priors) based credible intervals exclude zero. However, marginal selection unsatisfactory—prefer interpret subset joint collection variables—empirically inferior, substantially less power detect true effects (Kowal 2021, 2022a, 2022b). BayesSubsets seeks reinvigorate (Bayesian) subset selection. Given Bayesian regression model \\(M\\) predicts \\(y\\) \\(X\\), provide: Optimal linear coefficients subset variables, produces useful interpretable summary \\(M\\) (may complex/nonlinear); Regularization (.e., shrinkage) coefficients, inherited \\(M\\) helps guard variance-inflation; Uncertainty quantification coefficients, leveraging \\(M\\); acceptable family “near-optimal” subsets linear predictors match nearly match “best” (minimum cross-validated error) subset; Summaries acceptable family, including 1) smallest acceptable subset 2) variable importance metrics computes, variable \\(j\\), proportion acceptable subsets \\(j\\) appears. aggregate, features address limitations (classical) subset selection provide coherent Bayesian alternative anchored decision analysis. main contribution BayesSubsets computation summarization acceptable family, deemphasizes role single “best” subset instead advances broader perspective often many subsets highly competitive. especially true datasets correlated predictors, low signal--noise, small sample sizes. result, acceptable family counteracts selection instability traditionally afflicted (classical) subset selection.","code":""},{"path":[]},{"path":"https://drkowal.github.io/BayesSubsets/articles/BayesSubsets.html","id":"getting-started","dir":"Articles","previous_headings":"Using BayesSubsets","what":"Getting started","title":"Introduction to BayesSubsets","text":"begin installing loading package: example, consider simulated data correlated covariates \\(X\\) continuous outcome \\(y \\\\mathbb{R}\\):","code":"# devtools::install_github(\"drkowal/BayesSubsets\") library(BayesSubsets) # To reproduce: set.seed(123)   # Simulate some data: dat = simulate_lm(n = 200,   # number of observations                   p = 30,    # number of predictors                   p_sig = 5, # number of true signals                   SNR = 1    # signal-to-noise ratio )  # Store the data: y = dat$y; X = dat$X"},{"path":"https://drkowal.github.io/BayesSubsets/articles/BayesSubsets.html","id":"fitting-the-regression-model","dir":"Articles","previous_headings":"Using BayesSubsets","what":"Fitting the regression model","title":"Introduction to BayesSubsets","text":"first step fit Bayesian regression model. , use linear model horseshoe priors:","code":"library(bayeslm)  # Fit the Bayesian regression model: fit = bayeslm(y ~ X[,-1], # intercept already included               prior = 'horseshoe', # prior on regression coefficients               N = 10000, # MCMC samples to save               burnin = 5000 # initial samples to discard ) #> horseshoe prior  #> fixed running time 0.00286561 #> sampling time 0.612927"},{"path":"https://drkowal.github.io/BayesSubsets/articles/BayesSubsets.html","id":"computing-optimal-linear-coefficients","dir":"Articles","previous_headings":"Using BayesSubsets","what":"Computing optimal linear coefficients","title":"Introduction to BayesSubsets","text":"Given Bayesian regression model \\(M\\) subset covariates \\(S\\), compute optimal linear coefficients according Bayesian decision analysis. Kowal (2021) showed obtained simply projecting fitted values \\(\\hat y\\) \\(M\\) onto \\(X_S\\), .e., covariate matrix \\(X\\) restricted columns selected \\(S\\). example subset \\(S = \\{1,3,10\\}\\) squared error loss, following code computes optimal linear summary: emphasize two key points, First, classical linear regression \\(y\\) \\(X\\). see , recall \\(M\\), fitted values \\(\\hat y = X \\hat \\beta\\), \\(\\hat \\beta\\) posterior expectation regression coefficients Bayesian linear model. horseshoe prior \\(\\beta\\) shrink many coefficients toward zero, propagates regularization optimal coefficients projection . Thus, “fit---fit” resolves key criticism (classical) subset selection. Second, two-stage estimator classical sense, plug-estimates stage-one model plugged unknown parameter stage-two model. Instead, fully coherent decision analysis designed obtain certain type point estimate—just like posterior mean posterior median. example, optimal coefficients full set covariates \\(S = \\{1,\\ldots,p\\}\\) simply \\(\\hat \\beta\\)—usual posterior mean \\(M\\). linear summaries broad: can computed summarize Bayesian regression model \\(M\\) (just linear models) subset covariates \\(S\\).","code":"# Example subset: S_ex = c(1, 3, 10)   # Optimal coefficients: coef(lm(fitted(fit) ~ X[,S_ex] - 1)) #>  X[, S_ex]X1  X[, S_ex]X3 X[, S_ex]X10  #>   -0.8863355    0.4474228   -0.5731277"},{"path":"https://drkowal.github.io/BayesSubsets/articles/BayesSubsets.html","id":"uncertainty-quantification-for-the-linear-coefficients","dir":"Articles","previous_headings":"Using BayesSubsets","what":"Uncertainty quantification for the linear coefficients","title":"Introduction to BayesSubsets","text":"may also obtain posterior uncertainty quantification linear coefficients active (nonzero) \\(S\\). , project posterior predictive distribution onto \\(X_S\\) draw--draw, induces posterior predictive distribution linear coefficients model \\(M\\)—even though \\(M\\) need linear general. predictive draws automatically output bayeslm, run following code sample . also compute log-predictive densities later used predictive cross-validation. Now, can obtain posterior predictive samples linear coefficients \\(S\\), summarize posteriors using 95% credible intervals.","code":"# Extract the posterior predictive draws and lpd: temp = post_predict(post_y_hat = tcrossprod(fit$beta, X),                     post_sigma = fit$sigma,                     yy = y) post_y_pred = temp$post_y_pred post_lpd = temp$post_lpd # Posterior predictive draws of *all* coefficients: post_beta_s = proj_posterior(post_y_pred = post_y_pred,                                  XX = X,                                  sub_x = S_ex)  dim(post_beta_s) # the coefficients outside S_ex are fixed at zero #> [1] 10000    31  # Compute 95% credible intervals for the nonzero entries: t(apply(post_beta_s[,S_ex], 2,          quantile, c(0.05/2, 1 - 0.05/2))) #>            2.5%      97.5% #> X1  -1.30024730 -0.4645863 #> X3  -0.01006303  0.9018901 #> X10 -1.00739359 -0.1385361"},{"path":"https://drkowal.github.io/BayesSubsets/articles/BayesSubsets.html","id":"bayesian-subset-search","dir":"Articles","previous_headings":"Using BayesSubsets","what":"Bayesian subset search","title":"Introduction to BayesSubsets","text":"point, focused point interval (linear) summaries arbitrary yet fixed subset \\(S\\). However, often interested searching across subsets measuring predictive performances. , use model \\(M\\) output generate collection “candidate subsets” using decision analysis (Kowal, 2022a). small \\(p\\) may possible enumerate possible subsets. , screen “best” n_best = 50 models size according squared error loss. store Boolean matrix indicators: row individual subset, columns indicate variables included (TRUE) excluded (FALSE). \\(p \\gg 30\\), recommended use prescreen function, restricts search include num_to_keep possible active variables. makes branch--bound algorithm feasible large \\(p\\).","code":"indicators = branch_and_bound(yy = fitted(fit), # response is the fitted values                              XX = X,            # covariates                              n_best = 50        # restrict to the \"best\" 50 subsets of each size )  # Inspect: indicators[1:5, 1:10] #>            X1    X2    X3    X4    X5    X6    X7    X8    X9   X10 #> force_in TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>          TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>          TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE #>          TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>          TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  # Dimensions: dim(indicators) #> [1] 1412   31  # Summarize the model sizes: table(rowSums(indicators)) # note: intercept always included #>  #>  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  #>  1 30 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50  #> 27 28 29 30 31  #> 50 50 50 30  1"},{"path":"https://drkowal.github.io/BayesSubsets/articles/BayesSubsets.html","id":"the-acceptable-family-of-near-optimal-subsets","dir":"Articles","previous_headings":"Using BayesSubsets","what":"The acceptable family of “near-optimal” subsets","title":"Introduction to BayesSubsets","text":"large collection 1412 candidate subsets, seek filter acceptable family subsets, .e., “near-optimal” subsets predict well “best” subset. computed based 10-fold cross-validation, use --sample predictive distribution \\(M\\) provide uncertainty quantification predictive accuracy.  plot shows --sample predictive performance varies across subsets different sizes, specifically relative (% change) “best” subset (minimum cross-validated error; dashed gray vertical line). x-marks (usual) empirical cross-validated error, intervals leverage predictive distribution \\(M\\) quantify uncertainty --sample predictive performance. performance improves variables added, clear several smaller subsets highly competitive—especially accounting predictive uncertainty.","code":"# Compute the acceptable family: accept_info = accept_family(post_y_pred = post_y_pred,                             post_lpd = post_lpd,                             XX = X,                             indicators = indicators,                             yy = y,                             post_y_hat = tcrossprod(fit$beta, X)) # How many subsets are in the acceptable family? length(accept_info$all_accept) #> [1] 1163  # These are the rows of `indicators` that belong to the acceptable family: head(accept_info$all_accept) #> [1] 182 232 233 234 235 236  # An example acceptable subset: ex_accept = accept_info$all_accept[1] which(indicators[ex_accept,]) #> X1 X2 X3 X4 X5 X6  #>  1  2  3  4  5  6"},{"path":"https://drkowal.github.io/BayesSubsets/articles/BayesSubsets.html","id":"subset-selection-the-smallest-acceptable-subset","dir":"Articles","previous_headings":"Using BayesSubsets","what":"Subset selection: the smallest acceptable subset","title":"Introduction to BayesSubsets","text":"wish select single subset, compelling representative acceptable family smallest acceptable subset. choice favors parsimony, membership acceptable family implies meets high standard predictive accuracy. previous plot, select smallest subset intervals include zero (solid gray vertical line). “best” subset minimum cross-validation often includes many extraneous variables, well-known (undesirable) byproduct cross-validation. reference, true model size 6. Clearly, “best” subset unsatisfactory. Returning smallest acceptable subset, can obtain posterior samples credible intervals coefficients :","code":"# Simplest acceptable subset: beta_hat_small = accept_info$beta_hat_small  # Which coefficients are nonzero: S_small = which(beta_hat_small != 0)  # How many coefficients are nonzero: length(S_small) #> [1] 6 # Acceptable subset that minimizes CV error: beta_hat_min = accept_info$beta_hat_min  # Typically much larger (and often too large...) sum(beta_hat_min != 0) #> [1] 19 # Draws from the posterior predictive distribution post_beta_small = proj_posterior(post_y_pred = post_y_pred,                                  XX = X,                                  sub_x = S_small)  # Compute 95% credible intervals for the nonzero entries: t(apply(post_beta_small[,S_small], 2,          quantile, c(0.05/2, 1 - 0.05/2))) #>          2.5%      97.5% #> X1 -1.4434868 -0.6028323 #> X2  0.3398519  1.6808732 #> X3  0.3738225  1.3484581 #> X4  0.1798377  1.5472092 #> X5 -1.6691666 -0.5500480 #> X6 -1.4600657 -0.2625247"},{"path":"https://drkowal.github.io/BayesSubsets/articles/BayesSubsets.html","id":"variable-importance-from-acceptable-subsets","dir":"Articles","previous_headings":"Using BayesSubsets","what":"Variable importance from acceptable subsets","title":"Introduction to BayesSubsets","text":"Another useful summary acceptable family variable importance, reports, variable \\(j\\), proportion acceptable subsets \\(j\\) appears. particularly interested distinguishing among variables occur , , acceptable subsets, provides insight variables indispensable (“keystone covariates”) variables part “predictively plausible” explanation.  can see quite variables appear almost acceptable subsets: 6 variables belong least 95% acceptable subsets. also 6 variables belong fewer 20% acceptable subsets. Note covariates highly correlated simulated example (\\(p\\) moderate), reasonable expect many covariates roughly interchangeable terms predictive accuracy.","code":"# Variable importance: proportion of *acceptable subsets* in which each variable appears vi_e = var_imp(indicators = indicators,                all_accept = accept_info$all_accept)$vi_inc  # \"Keystone covariates\" that appear in *all* acceptable families: which(vi_e == 1) #> 1 4  #> 1 4  # Irrelevant covariates that appear in *no* acceptable families: which(vi_e == 0)  #> named integer(0)  # Visualize: barplot(vi_e[order(vi_e, (ncol(X):1))], # order...         horiz = TRUE,          main = paste('Variable importance for the acceptable family')) abline(v = 1)"},{"path":"https://drkowal.github.io/BayesSubsets/articles/BayesSubsets.html","id":"comparing-with-traditional-posterior-summaries","dir":"Articles","previous_headings":"Using BayesSubsets","what":"Comparing with traditional posterior summaries","title":"Introduction to BayesSubsets","text":"Typically, Bayesian linear regression report posterior expectations 95% posterior credible intervals regression coefficients \\(\\beta\\). plot together point interval estimates smallest acceptable model:  traditional model summaries completely dense: point estimates \\(\\hat \\beta\\) nonzero covariates. comparison, point estimates smallest acceptable subset sparse, 6 active coefficients. design, smallest acceptable subset reports interval estimates active coefficients. example, intervals narrower smallest acceptable subset, traditional intervals produce large intervals even truly zero coefficients. Finally, verify predictive accuracy recover true regression surface, \\(X\\beta_{true}\\): , smallest acceptable subset exceptional predictor. However, keep mind optimized near-optimal predictive accuracy along simplicity (sparsity). often potent combination, make claims uniformly outperform members acceptable family.","code":"# RMSE from posterior mean: sqrt(mean((dat$Ey_true - X%*%beta_hat)^2)) #> [1] 0.5382842  # RMSE from smallest acceptable subset: sqrt(mean((dat$Ey_true - X%*%beta_hat_small)^2)) #> [1] 0.1924557"},{"path":"https://drkowal.github.io/BayesSubsets/articles/BayesSubsets.html","id":"conclusion","dir":"Articles","previous_headings":"Using BayesSubsets","what":"Conclusion","title":"Introduction to BayesSubsets","text":"sought demonstrate BayesSubsets useful accompaniment usual Bayesian regression workflow. Given Bayesian regression model \\(M\\), shown compute Optimal linear summaries subset covariates; Accompanying uncertainty quantification via posterior (predictive) distributions intervals; Bayesian subset search using decision analysis; acceptable family near-optimal subsets; Key summaries acceptable family, including smallest acceptable subset variable importance metric. single subset required parsimony valued, recommend smallest acceptable subset. However, caution overreliance single subset without compelling motivation. key contribution acceptable family identifies many competing explanations (subsets) nearly indistinguishable predictive accuracy. purely predictive perspective, completely rule member acceptable family. Thus, recommend reporting variable importance default, variable-specific summary acceptable family.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/articles/Binary-target.html","id":"targeted-prediction","dir":"Articles","previous_headings":"","what":"Targeted prediction","title":"Targeted Prediction and Binary Outcomes","text":"Given regression model \\(M\\) data \\((X, y)\\), often priority summarize (interpret) \\(M\\). , primary decision task scientific inquiry frequently described functional \\(h(y)\\), exceedance threshold. Targeted prediction customizes summarization \\(M\\) specifically \\(h\\), including linear coefficient estimates/intervals subset selection (Kowal, 2021). achieved using Bayesian decision analysis \\(h(\\tilde{y})\\), \\(\\tilde{y}\\) posterior predictive variable \\(M\\). several reasons use targeted prediction: can summarize \\(M\\) multiple functionals \\(h_1, h_2, \\ldots\\) without refitting model \\(h_k\\); model \\(M\\) provides regularization \\(\\tilde{y}\\) thus \\(h(\\tilde{y})\\), offers downstream benefits point/interval estimation selection (e.g., \\(y\\) functional data \\(h\\) maximum; see Kowal, 2021 examples); generally, functional may written \\(h(\\theta, \\tilde{y})\\) model \\(M\\) parameters \\(\\theta\\), thus depends unobservables (computed directly data \\(y\\)).","code":""},{"path":"https://drkowal.github.io/BayesSubsets/articles/Binary-target.html","id":"binary-outcomes","dir":"Articles","previous_headings":"","what":"Binary outcomes","title":"Targeted Prediction and Binary Outcomes","text":"focus binary functionals \\(h:\\mathbb{R}\\\\{0,1\\}\\), \\(h(\\tilde{y})\\) \\(h(y)\\) binary. Thus, example also highlight capabilities BayesSubsets binary outcomes. Specifically, decision analysis requires choice loss function. Implementations available cross-entropy loss misclassification rate; focus former.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/articles/Binary-target.html","id":"getting-started","dir":"Articles","previous_headings":"","what":"Getting started","title":"Targeted Prediction and Binary Outcomes","text":"begin installing loading package: example, consider simulated data correlated covariates \\(X\\) continuous outcome \\(y \\\\mathbb{R}\\): Next, fit Bayesian linear model. output bayeslm include posterior predictive draws log-predictive density evaluations, compute well. point, mention functional \\(h\\): observed data \\((X, y)\\) fit Bayesian model \\(M\\). Importantly, can define many different options \\(h\\) point forward, without need refit \\(M\\).","code":"# devtools::install_github(\"drkowal/BayesSubsets\") library(BayesSubsets) # To reproduce: set.seed(123)   # Simulate some data: dat = simulate_lm(n = 200,   # number of observations                   p = 10,    # number of predictors                   p_sig = 5, # number of true signals                   SNR = 1    # signal-to-noise ratio )  # Store the data: y = dat$y; X = dat$X # Package for efficient Bayesian linear regression: library(bayeslm)  # Fit the Bayesian regression model: fit = bayeslm(y ~ X[,-1], # intercept already included               N = 10000, # MCMC samples to save               burnin = 5000 # initial samples to discard ) #> horseshoe prior  #> fixed running time 0.00209821 #> sampling time 0.210797  # Extract the posterior predictive draws and lpd: temp = post_predict(post_y_hat = tcrossprod(fit$beta, X),                     post_sigma = fit$sigma,                     yy = y) post_y_pred = temp$post_y_pred post_lpd = temp$post_lpd"},{"path":"https://drkowal.github.io/BayesSubsets/articles/Binary-target.html","id":"defining-the-functional","dir":"Articles","previous_headings":"","what":"Defining the functional","title":"Targeted Prediction and Binary Outcomes","text":"define \\(h\\) exceedance threshold: \\[ h(t) = 1(t \\ge \\tau) \\] \\(\\tau\\) specified advance. fix \\(\\tau\\) 90th quantile: critical term posterior predictive distribution \\(h(\\tilde{y})\\), expectation. easy compute: subtle yet important point fitted values \\(\\bar{h} = \\mathbb{E}\\{h(\\tilde{y}) | y\\} \\[0,1]\\) continuous, \\(h(y) \\\\{0,1\\}\\) binary. well-specified model \\(M\\), \\(\\bar{h}\\) may informative \\(h(y)\\) lies along continuum.","code":"tau = quantile(y, 0.9) h = function(t){   1.0*I(t >= tau) } # Posterior predictive draws of h: post_h_pred = h(post_y_pred)  # Fitted values: h_bar = colMeans(post_h_pred)"},{"path":"https://drkowal.github.io/BayesSubsets/articles/Binary-target.html","id":"computing-optimal-linear-coefficients","dir":"Articles","previous_headings":"","what":"Computing optimal linear coefficients","title":"Targeted Prediction and Binary Outcomes","text":"Given Bayesian regression model \\(M\\) subset covariates \\(S\\), compute optimal linear coefficients according Bayesian decision analysis. Kowal (2021) showed coefficients can computed (cross-entropy loss) using “fit---fit”, specifically logistic regression model response \\(\\bar{h}\\) covariates \\(X_S\\), .e., covariate matrix \\(X\\) restricted columns selected \\(S\\). example subset \\(S = \\{1,3,10\\}\\) cross-entropy loss, following code computes optimal linear summary:","code":"# Example subset: S_ex = c(1, 3, 10)   # Optimal coefficients: coef(glm(h_bar ~ X[,S_ex] - 1), family = binomial()) #>  X[, S_ex]X1  X[, S_ex]X3 X[, S_ex]X10  #>   0.11654316   0.11131220  -0.02752669"},{"path":"https://drkowal.github.io/BayesSubsets/articles/Binary-target.html","id":"uncertainty-quantification-for-the-linear-coefficients","dir":"Articles","previous_headings":"","what":"Uncertainty quantification for the linear coefficients","title":"Targeted Prediction and Binary Outcomes","text":"may also obtain posterior uncertainty quantification linear coefficients active (nonzero) \\(S\\). , compute logistic regression posterior predictive draw \\(h(\\tilde{y})\\), induces posterior predictive distribution linear coefficients model \\(M\\)—even though \\(M\\) model binary data. summarize posteriors using 95% credible intervals.","code":"# Posterior predictive draws of *all* coefficients: sub_sims = sample(1:nrow(post_h_pred), 1000) # just use 1000 draws post_beta_s = proj_posterior(post_y_pred = post_h_pred[sub_sims,],                               XX = X,                              sub_x = S_ex,                              use_ols = FALSE)  dim(post_beta_s) # the coefficients outside S_ex are fixed at zero #> [1] 1000   11  # Compute 95% credible intervals for the nonzero entries: t(apply(post_beta_s[,S_ex], 2,          quantile, c(0.05/2, 1 - 0.05/2))) #>           2.5%      97.5% #> X1  -3.2129778 -1.8746403 #> X3   0.6244174  2.2529052 #> X10 -0.9761212  0.2704743"},{"path":"https://drkowal.github.io/BayesSubsets/articles/Binary-target.html","id":"bayesian-subset-search","dir":"Articles","previous_headings":"","what":"Bayesian subset search","title":"Targeted Prediction and Binary Outcomes","text":"search across candidate subsets, use branch--bound algorithm weighted least squares approximation cross-entropy loss. approximation uses logit \\(\\bar{h}\\) response \\(\\bar{h}(1 - \\bar{h})\\) weights. small \\(p\\) may possible enumerate possible subsets. , screen “best” n_best = 50 models size according squared error loss. store Boolean matrix indicators: row individual subset, columns indicate variables included (TRUE) excluded (FALSE). also check make sure \\(\\bar{h} \\ne 0\\) \\(\\bar{h} \\ne 1\\), can occur numerically creates problems logit call.","code":"# Make sure we do not have any zeros or ones: h_bar[h_bar == 0] = min(h_bar[h_bar != 0]) # set to the non-zero min h_bar[h_bar == 1] = max(h_bar[h_bar != 1]) # set to the non-one max  indicators = branch_and_bound(yy = log(h_bar/(1 - h_bar)), # response is the logit of h_bar                              XX = X,            # covariates                              wts = h_bar*(1 - h_bar), # weights for weighted least squares                              n_best = 50        # restrict to the \"best\" 50 subsets of each size )  # Inspect: indicators[1:5, 1:10] #>            X1    X2    X3    X4    X5    X6    X7    X8    X9   X10 #> force_in TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>          TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>          TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE #>          TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>          TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  # Dimensions: dim(indicators) #> [1] 362  11  # Summarize the model sizes: table(rowSums(indicators)) # note: intercept always included #>  #>  1  2  3  4  5  6  7  8  9 10 11  #>  1 10 45 50 50 50 50 50 45 10  1"},{"path":"https://drkowal.github.io/BayesSubsets/articles/Binary-target.html","id":"the-acceptable-family-of-near-optimal-subsets","dir":"Articles","previous_headings":"","what":"The acceptable family of “near-optimal” subsets","title":"Targeted Prediction and Binary Outcomes","text":"large collection 362 candidate subsets, seek filter acceptable family subsets, .e., “near-optimal” subsets predict well “best” subset. computed based 10-fold cross-validation, use --sample predictive distribution \\(M\\) provide uncertainty quantification predictive accuracy.  plot shows --sample predictive performance varies across subsets different sizes, specifically relative (% change) “best” subset (minimum cross-validated error; dashed gray vertical line). x-marks (usual) empirical cross-validated error, intervals leverage predictive distribution \\(M\\) quantify uncertainty --sample predictive performance. performance improves variables added, clear several smaller subsets highly competitive—especially accounting predictive uncertainty.","code":"# Compute the acceptable family: accept_info = accept_family_binary(post_y_pred = post_h_pred,                                    post_lpd = post_lpd,                                    XX = X,                                    indicators = indicators,                                    loss_type = \"cross-ent\",                                    yy = h(y)) # How many subsets are in the acceptable family? length(accept_info$all_accept) #> [1] 257  # These are the rows of `indicators` that belong to the acceptable family: head(accept_info$all_accept) #> [1] 57 58 59 60 63 64  # An example acceptable subset: ex_accept = accept_info$all_accept[1] which(indicators[ex_accept,]) #> X1 X2 X5 X6  #>  1  2  5  6"},{"path":"https://drkowal.github.io/BayesSubsets/articles/Binary-target.html","id":"subset-selection-the-smallest-acceptable-subset","dir":"Articles","previous_headings":"","what":"Subset selection: the smallest acceptable subset","title":"Targeted Prediction and Binary Outcomes","text":"wish select single subset, compelling representative acceptable family smallest acceptable subset. choice favors parsimony, membership acceptable family implies meets high standard predictive accuracy. previous plot, select smallest subset intervals include zero (solid gray vertical line). can obtain posterior samples credible intervals coefficients :","code":"# Simplest acceptable subset: beta_hat_small = accept_info$beta_hat_small  # Which coefficients are nonzero: S_small = which(beta_hat_small != 0)  # How many coefficients are nonzero: length(S_small) #> [1] 4 # Draws from the posterior predictive distribution post_beta_small = proj_posterior(post_y_pred = post_h_pred[sub_sims,], # just use 1000 draws                                  XX = X,                                  sub_x = S_small,                                  use_ols = FALSE)  # Compute 95% credible intervals for the nonzero entries: t(apply(post_beta_small[,S_small], 2,          quantile, c(0.05/2, 1 - 0.05/2))) #>         2.5%       97.5% #> X1 -3.711372 -2.01259590 #> X2  1.019795  2.73711706 #> X5 -1.289039 -0.08855023 #> X6 -1.497176 -0.06492849"},{"path":"https://drkowal.github.io/BayesSubsets/articles/Binary-target.html","id":"variable-importance-from-acceptable-subsets","dir":"Articles","previous_headings":"","what":"Variable importance from acceptable subsets","title":"Targeted Prediction and Binary Outcomes","text":"Another useful summary acceptable family variable importance, reports, variable \\(j\\), proportion acceptable subsets \\(j\\) appears. particularly interested distinguishing among variables occur , , acceptable subsets, provides insight variables indispensable (“keystone covariates”) variables part “predictively plausible” explanation.  variables belong , , acceptable subsets.","code":"# Variable importance: proportion of *acceptable subsets* in which each variable appears vi_e = var_imp(indicators = indicators,                all_accept = accept_info$all_accept)$vi_inc  # \"Keystone covariates\" that appear in *all* acceptable families: which(vi_e == 1) #> 1  #> 1  # Irrelevant covariates that appear in *no* acceptable families: which(vi_e == 0)  #> named integer(0)  # Visualize: barplot(vi_e[order(vi_e, (ncol(X):1))], # order...         horiz = TRUE,          main = paste('Variable importance for the acceptable family')) abline(v = 1)"},{"path":"https://drkowal.github.io/BayesSubsets/articles/Binary-target.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"Targeted Prediction and Binary Outcomes","text":"Even various functionals \\(h\\) /binary outcomes, pipeline nearly identical simpler continuous outcome case. binary outcomes, code moderately slower: repeated evaluations glm less efficient lm, needed 1) posterior predictive uncertainty quantification via proj_posterior (use_ols = FALSE) 2) predictive evaluation construct acceptable family via accept_family_binary. However, need refit \\(M\\) different choices \\(h\\) (e.g., varying threshold \\(\\tau\\)), can substantial time-saver \\(M\\) complex (e.g, functional data regression model). Finally, note \\(M\\) binary regression model \\(h(t) = t\\) unnecessary, can replace post_h_pred post_y_pred throughout (y simply replaces h(y)).","code":""},{"path":"https://drkowal.github.io/BayesSubsets/articles/High-dim.html","id":"high-dimensional-data","dir":"Articles","previous_headings":"","what":"High-dimensional data","title":"BayesSubsets with high-dimensional data","text":"document revisits BayesSubsets subset search, selection, summarization. particular, consider case \\(p > n\\) \\(p = 400\\) large. traditionally infeasible subset selection, describe pre-screening tools deploy order apply BayesSubsets.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/articles/High-dim.html","id":"getting-started","dir":"Articles","previous_headings":"","what":"Getting started","title":"BayesSubsets with high-dimensional data","text":"begin installing loading package: example, consider simulated data correlated covariates \\(X\\) continuous outcome \\(y \\\\mathbb{R}\\). , \\(p=400\\) covariates, \\(5\\) true signals.","code":"# devtools::install_github(\"drkowal/BayesSubsets\") library(BayesSubsets) # To reproduce: set.seed(123)   # Simulate some data: dat = simulate_lm(n = 200,   # number of observations                   p = 400,    # number of predictors                   p_sig = 5, # number of true signals                   SNR = 1    # signal-to-noise ratio )  # Store the data: y = dat$y; X = dat$X"},{"path":"https://drkowal.github.io/BayesSubsets/articles/High-dim.html","id":"fitting-the-regression-model","dir":"Articles","previous_headings":"","what":"Fitting the regression model","title":"BayesSubsets with high-dimensional data","text":"first step fit Bayesian regression model. , use linear model horseshoe priors:","code":"library(bayeslm)  # Fit the Bayesian regression model: fit = bayeslm(y ~ X[,-1], # intercept already included               prior = 'horseshoe', # prior on regression coefficients               N = 10000, # MCMC samples to save               burnin = 5000, # initial samples to discard               singular = TRUE # necessary for p > n ) #> horseshoe prior  #> fixed running time 2.01974 #> sampling time 19.7884"},{"path":"https://drkowal.github.io/BayesSubsets/articles/High-dim.html","id":"computing-optimal-linear-coefficients","dir":"Articles","previous_headings":"","what":"Computing optimal linear coefficients","title":"BayesSubsets with high-dimensional data","text":"Given Bayesian regression model \\(M\\) subset covariates \\(S\\), compute optimal linear coefficients according Bayesian decision analysis. Kowal (2021) showed obtained simply projecting fitted values \\(\\hat y\\) \\(M\\) onto \\(X_S\\), .e., covariate matrix \\(X\\) restricted columns selected \\(S\\). example subset \\(S = \\{1,3,10\\}\\) squared error loss, following code computes optimal linear summary:","code":"# Example subset: S_ex = c(1, 3, 10)   # Optimal coefficients: coef(lm(fitted(fit) ~ X[,S_ex] - 1)) #>  X[, S_ex]X1  X[, S_ex]X3 X[, S_ex]X10  #>   -0.6296313    1.0597193   -0.2288200"},{"path":"https://drkowal.github.io/BayesSubsets/articles/High-dim.html","id":"uncertainty-quantification-for-the-linear-coefficients","dir":"Articles","previous_headings":"","what":"Uncertainty quantification for the linear coefficients","title":"BayesSubsets with high-dimensional data","text":"may also obtain posterior uncertainty quantification linear coefficients active (nonzero) \\(S\\). , project posterior predictive distribution onto \\(X_S\\) draw--draw, induces posterior predictive distribution linear coefficients model \\(M\\)—even though \\(M\\) need linear general. predictive draws automatically output bayeslm, run following code sample . also compute log-predictive densities later used predictive cross-validation. Now, can obtain posterior predictive samples linear coefficients \\(S\\), summarize posteriors using 95% credible intervals.","code":"# Extract the posterior predictive draws and lpd: temp = post_predict(post_y_hat = tcrossprod(fit$beta, X),                     post_sigma = fit$sigma,                     yy = y) post_y_pred = temp$post_y_pred post_lpd = temp$post_lpd # Posterior predictive draws of *all* coefficients: post_beta_s = proj_posterior(post_y_pred = post_y_pred,                                  XX = X,                                  sub_x = S_ex)  dim(post_beta_s) # the coefficients outside S_ex are fixed at zero #> [1] 10000   401  # Compute 95% credible intervals for the nonzero entries: t(apply(post_beta_s[,S_ex], 2,          quantile, c(0.05/2, 1 - 0.05/2))) #>           2.5%      97.5% #> X1  -1.1040525 -0.1544037 #> X3   0.5828605  1.5262942 #> X10 -0.6296572  0.1535756"},{"path":"https://drkowal.github.io/BayesSubsets/articles/High-dim.html","id":"bayesian-subset-search","dir":"Articles","previous_headings":"","what":"Bayesian subset search","title":"BayesSubsets with high-dimensional data","text":"point, focused point interval (linear) summaries arbitrary yet fixed subset \\(S\\). However, often interested searching across subsets measuring predictive performances. , use model \\(M\\) output generate collection “candidate subsets” using decision analysis (Kowal, 2022a). make search feasible large \\(p\\), first pre-screen \\(k \\ll p\\) covariates. using posterior draws \\(\\beta\\) \\(M\\), variations available \\(M\\) nonlinear. Specifically, retain top \\(k\\) covariates effect size. \\(k\\) covariates permitted active. makes branch--bound algorithm feasible. , vignettes, screen “best” n_best = 50 models size according squared error loss. store Boolean matrix indicators: row individual subset, columns indicate variables included (TRUE) excluded (FALSE).","code":"# Allowable covariates: to_consider = prescreen(fit$beta, num_to_keep = 30)  # Exclude the rest: to_exclude = (1:ncol(X))[-to_consider] indicators = branch_and_bound(yy = fitted(fit), # response is the fitted values                              XX = X,            # covariates                              n_best = 50,       # restrict to the \"best\" 15 subsets of each size                              to_include = 1,    # keep the intercept always                              to_exclude = to_exclude # pre-screened )  # Inspect: indicators[1:5, 1:10] #>            X1    X2    X3    X4    X5    X6    X7    X8    X9   X10 #> force_in TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>          TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>          TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE #>          TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE #>          TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  # Dimensions: dim(indicators) #> [1] 1360  401  # Summarize the model sizes: table(rowSums(indicators)) # note: intercept always included #>  #>  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  #>  1 29 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50  #> 27 28 29 30  #> 50 50 29  1"},{"path":"https://drkowal.github.io/BayesSubsets/articles/High-dim.html","id":"the-acceptable-family-of-near-optimal-subsets","dir":"Articles","previous_headings":"","what":"The acceptable family of “near-optimal” subsets","title":"BayesSubsets with high-dimensional data","text":"large collection 1360 candidate subsets, seek filter acceptable family subsets, .e., “near-optimal” subsets predict well “best” subset. computed based 10-fold cross-validation, use --sample predictive distribution \\(M\\) provide uncertainty quantification predictive accuracy.  plot shows --sample predictive performance varies across subsets different sizes, specifically relative (% change) “best” subset (minimum cross-validated error; dashed gray vertical line). x-marks (usual) empirical cross-validated error, intervals leverage predictive distribution \\(M\\) quantify uncertainty --sample predictive performance. performance improves variables added, clear several smaller subsets highly competitive—especially accounting predictive uncertainty.","code":"# Compute the acceptable family: accept_info = accept_family(post_y_pred = post_y_pred,                             post_lpd = post_lpd,                             XX = X,                             indicators = indicators,                             yy = y,                             post_y_hat = tcrossprod(fit$beta, X)) # How many subsets are in the acceptable family? length(accept_info$all_accept) #> [1] 1178  # These are the rows of `indicators` that belong to the acceptable family: head(accept_info$all_accept) #> [1] 131 132 181 182 183 184  # An example acceptable subset: ex_accept = accept_info$all_accept[1] which(indicators[ex_accept,]) #>   X1   X3   X4   X6 X173  #>    1    3    4    6  173"},{"path":"https://drkowal.github.io/BayesSubsets/articles/High-dim.html","id":"subset-selection-the-smallest-acceptable-subset","dir":"Articles","previous_headings":"","what":"Subset selection: the smallest acceptable subset","title":"BayesSubsets with high-dimensional data","text":"wish select single subset, compelling representative acceptable family smallest acceptable subset. choice favors parsimony, membership acceptable family implies meets high standard predictive accuracy. previous plot, select smallest subset intervals include zero (solid gray vertical line). “best” subset minimum cross-validation often includes many extraneous variables, well-known (undesirable) byproduct cross-validation. reference, true model size 6. Clearly, “best” subset unsatisfactory. Returning smallest acceptable subset, can obtain posterior samples credible intervals coefficients :","code":"# Simplest acceptable subset: beta_hat_small = accept_info$beta_hat_small  # Which coefficients are nonzero: S_small = which(beta_hat_small != 0)  # How many coefficients are nonzero: length(S_small) #> [1] 5 # Acceptable subset that minimizes CV error: beta_hat_min = accept_info$beta_hat_min  # Typically much larger (and often too large...) sum(beta_hat_min != 0) #> [1] 27 # Draws from the posterior predictive distribution post_beta_small = proj_posterior(post_y_pred = post_y_pred,                                  XX = X,                                  sub_x = S_small)  # Compute 95% credible intervals for the nonzero entries: t(apply(post_beta_small[,S_small], 2,          quantile, c(0.05/2, 1 - 0.05/2))) #>            2.5%      97.5% #> X1   -1.3650754 -0.3892930 #> X3    0.5819645  1.5333044 #> X4    0.7520160  1.7982194 #> X6   -1.6517656 -0.7025616 #> X173 -1.4220912 -0.4092995"},{"path":"https://drkowal.github.io/BayesSubsets/articles/High-dim.html","id":"variable-importance-from-acceptable-subsets","dir":"Articles","previous_headings":"","what":"Variable importance from acceptable subsets","title":"BayesSubsets with high-dimensional data","text":"variable importance reports, variable \\(j\\), proportion acceptable subsets \\(j\\) appears. However, must interpret additional caution: pre-screening procedure eliminates certain variables consideration acceptable family, thus variables assigned importance value zero. result, informative focus variables appear acceptable subsets.  expected, variables (specifically, 371) end value exactly zero, due pre-screening procedure. However, among candidate variables, still variability variable importances, quantified summary output:","code":"# Variable importance: proportion of *acceptable subsets* in which each variable appears vi_e = var_imp(indicators = indicators,                all_accept = accept_info$all_accept)$vi_inc  # \"Keystone covariates\" that appear in *all* acceptable families: which(vi_e == 1) #> 1 3 4 6  #> 1 3 4 6  # Visualize: barplot(vi_e[order(vi_e, (ncol(X):1))], # order...         horiz = TRUE,          main = paste('Variable importance for the acceptable family')) abline(v = 1) # Summary stats for the nonzero VIs: summary(vi_e[vi_e != 0]) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>  0.1036  0.2924  0.5577  0.5755  0.8686  1.0000"},{"path":"https://drkowal.github.io/BayesSubsets/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Dan Kowal. Author, maintainer, copyright holder.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kowal D (2023). BayesSubsets: Bayesian Subset Selection Variable Importance. https://github.com/drkowal/BayesSubsets, https://drkowal.github.io/BayesSubsets/.","code":"@Manual{,   title = {BayesSubsets: Bayesian Subset Selection and Variable Importance},   author = {Dan Kowal},   year = {2023},   note = {https://github.com/drkowal/BayesSubsets, https://drkowal.github.io/BayesSubsets/}, }"},{"path":"https://drkowal.github.io/BayesSubsets/index.html","id":"bayessubsets","dir":"","previous_headings":"","what":"Bayesian Subset Selection and Variable Importance","title":"Bayesian Subset Selection and Variable Importance","text":"BayesSubsets provides Bayesian subset selection variety Bayesian regression models. include: Bayesian regression y ∈ ℝ y ∈ {0, 1} (n × p) covariates X (Kowal, 2022a) Bayesian linear mixed models regress Y (m × n) X, observe m repeated measurements subject  = 1, …, n (Kowal, 2022b) Targeted prediction h(ỹ), h known functional describes key outcome interest (e.g., y continuous, h indicator exceedance threshold); multiple functionals can considered single model (Kowal, 2021). specific cases explored vignettes. Bayesian regression model M, BayesSubsets provides: Optimal linear coefficients (uncertainty quantification) given subset covariates; acceptable family subsets near-optimal linear predictors relative “best” subset (according cross-validation); smallest acceptable subset, prioritizes parsimony maintaining predictive accuracy; variable importance acceptable family, provides variable-specific summaries near-optimal subsets. smallest acceptable subset reasonable default choice subset selection. However, caution overreliance single subset without compelling motivation. key contribution acceptable family identifies many competing explanations (subsets) nearly indistinguishable predictive accuracy. purely predictive perspective, completely rule member acceptable family. , recommend including variable importance metric summarize many near-optimal linear predictors.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Bayesian Subset Selection and Variable Importance","text":"can install development version BayesSubsets GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"drkowal/BayesSubsets\")"},{"path":"https://drkowal.github.io/BayesSubsets/index.html","id":"example-use","dir":"","previous_headings":"","what":"Example use","title":"Bayesian Subset Selection and Variable Importance","text":"illustrate use BayesSubsets, present example simulated data Bayesian linear model. vignettes explore cases detail. First, load package: simulate data: Next, fit Bayesian linear model. output bayeslm include posterior predictive draws log-predictive density evaluations, compute well. Using model output, enumerate collection “candidate subsets”. small p may possible include possible subsets. , screen “best” n_best = 50 models size according squared error loss. store Boolean matrix indicators: row individual subset, columns indicate variables included (TRUE) excluded (FALSE). collection 361 candidate subsets, seek filter acceptable family subsets, .e., “near-optimal” subsets predict well “best” subset. computed based 10-fold cross-validation, use --sample predictive distribution M provide uncertainty quantification predictive accuracy.  plot shows --sample predictive performance varies across subsets different sizes, specifically relative (% change) “best” subset (minimum cross-validated error; dashed gray vertical line). x-marks (usual) empirical cross-validated error, intervals leverage predictive distribution M quantify uncertainty --sample predictive performance. performance improves variables added, clear several smaller subsets highly competitive—especially accounting predictive uncertainty. wish select single subset, compelling representative acceptable family smallest acceptable subset. choice favors parsimony, membership acceptable family implies meets high standard predictive accuracy. previous plot, select smallest subset intervals include zero (solid gray vertical line). “best” subset minimum cross-validation often includes many extraneous variables, well-known (undesirable) byproduct cross-validation. reference, true model size 6. Returning smallest acceptable subset, can obtain posterior samples credible intervals coefficients : Another useful summary acceptable family variable importance, reports, variable j, proportion acceptable subsets j appears. particularly interested distinguishing among variables occur , , acceptable subsets, provides insight variables indispensable (“keystone covariates”) variables part “predictively plausible” explanation.  variable appears quite acceptable subsets, unsurprising: covariates moderately correlated, reasonable expect roughly interchangeable terms predictive accuracy. Finally, compare point interval summaries smallest acceptable subset traditional point interval summaries Bayesian linear model: posterior mean 95% credible intervals β.  traditional model summaries completely dense: point estimates β̂ nonzero covariates. comparison, point estimates smallest acceptable subset sparse, 5 active coefficients. sets posterior summaries track true coefficients reasonably well. Additional documentation examples available https://drkowal.github.io/BayesSubsets/.","code":"library(BayesSubsets) # To reproduce: set.seed(123)   # Simulate some data: dat = simulate_lm(n = 200,   # number of observations                   p = 10,    # number of predictors                   p_sig = 5, # number of true signals                   SNR = 1    # signal-to-noise ratio )  # Store the data: y = dat$y; X = dat$X # Package for efficient Bayesian linear regression: library(bayeslm)  # Fit the Bayesian regression model: fit = bayeslm(y ~ X[,-1], # intercept already included               N = 10000, # MCMC samples to save               burnin = 5000 # initial samples to discard ) #> horseshoe prior  #> fixed running time 0.00316339 #> sampling time 0.270838  # Extract the posterior predictive draws and lpd: temp = post_predict(post_y_hat = tcrossprod(fit$beta, X),                     post_sigma = fit$sigma,                     yy = y) post_y_pred = temp$post_y_pred post_lpd = temp$post_lpd indicators = branch_and_bound(yy = fitted(fit), # response is the fitted values                              XX = X,            # covariates                              n_best = 50        # restrict to the \"best\" 50 subsets of each size )  # Inspect: indicators[1:5, 1:10] #>        X1    X2    X3    X4    X5    X6    X7    X8    X9   X10 #> [1,] TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [2,] TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE #> [3,] TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [4,] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE #> [5,] TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  # Dimensions: dim(indicators) #> [1] 361  11  # Summarize the model sizes: table(rowSums(indicators)) # note: intercept always included #>  #>  2  3  4  5  6  7  8  9 10 11  #> 10 45 50 50 50 50 50 45 10  1 # Compute the acceptable family: accept_info = accept_family(post_y_pred = post_y_pred,                             post_lpd = post_lpd,                             XX = X,                             indicators = indicators,                             yy = y,                             post_y_hat = tcrossprod(fit$beta, X)) # How many subsets are in the acceptable family? length(accept_info$all_accept) #> [1] 103  # These are the rows of `indicators` that belong to the acceptable family: head(accept_info$all_accept) #> [1] 106 156 157 158 159 160  # An example acceptable subset: ex_accept = accept_info$all_accept[1] which(indicators[ex_accept,]) #> X1 X3 X4 X5 X6  #>  1  3  4  5  6 # Simplest acceptable subset: beta_hat_small = accept_info$beta_hat_small  # Which coefficients are nonzero: S_small = which(beta_hat_small != 0)  # How many coefficients are nonzero: length(S_small) #> [1] 5 # Acceptable subset that minimizes CV error: beta_hat_min = accept_info$beta_hat_min  # Typically much larger (and often too large...) sum(beta_hat_min != 0) #> [1] 8 # Draws from the posterior predictive distribution post_beta_small = proj_posterior(post_y_pred = post_y_pred,                                  XX = X,                                  sub_x = S_small)  # Compute 95% credible intervals for the nonzero entries: t(apply(post_beta_small[,S_small], 2,          quantile, c(0.05/2, 1 - 0.05/2))) #>          2.5%      97.5% #> X1 -1.6918273 -0.8464020 #> X3  0.6238419  1.8385971 #> X4  1.0472439  2.1673297 #> X5 -1.3055990 -0.4155887 #> X6 -1.5278812 -0.5626135 # Variable importance: proportion of *acceptable subsets* in which each variable appears vi_e = var_imp(indicators = indicators,                all_accept = accept_info$all_accept)$vi_inc  # \"Keystone covariates\" that appear in *all* acceptable families: which(vi_e == 1) #> 1 4 6  #> 1 4 6  # Irrelevant covariates that appear in *no* acceptable families: which(vi_e == 0)  #> named integer(0)  # Visualize: barplot(vi_e[order(vi_e, (ncol(X):1))], # order...         horiz = TRUE,          main = paste('Variable importance for the acceptable family')) abline(v = 1)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the acceptable family of linear subsets — accept_family","title":"Compute the acceptable family of linear subsets — accept_family","text":"Given output Bayesian model candidate subsets, compute *acceptable family* subsets match nearly match predictive accuracy \"best\" subset. acceptable family may computed set covariate values XX; XX = X -sample points, cross-validation used assess --sample predictive performance.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the acceptable family of linear subsets — accept_family","text":"","code":"accept_family(   post_y_pred,   post_lpd = NULL,   XX,   yy = NULL,   indicators,   eps_level = 0.05,   eta_level = 0,   post_y_hat = NULL,   K = 10,   sir_frac = 0.5,   plot = TRUE )"},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the acceptable family of linear subsets — accept_family","text":"post_y_pred S x n matrix posterior predictive given XX covariate values post_lpd S evaluations log-likelihood computed posterior draw parameters (optional) XX n x p matrix covariates evaluate yy n-dimensional vector response variables (optional) indicators L x p matrix inclusion indicators (booleans) row denotes candidate subset eps_level probability required match predictive performance \"best\" model (eta_level) eta_level allowable margin ( \"best\" model post_y_hat S x n matrix posterior fitted values given XX covariate values (optional) K number cross-validation folds (optional) sir_frac fraction posterior samples use SIR (optional) plot logical; TRUE, include plot summarize predictive performance across candidate subsets","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the acceptable family of linear subsets — accept_family","text":"list containing following elements: all_accept: indices (.e., rows indicators) correspond acceptable subsets beta_hat_small linear coefficients smallest acceptable model beta_hat_min linear coefficients \"best\" acceptable model ell_small: index (.e., row indicators) smallest acceptable model ell_min: index (.e., row indicators) \"best\" acceptable model","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute the acceptable family of linear subsets — accept_family","text":"XX = X observed covariate values, post_lpd yy must provided. used compute cross-validated predictive empirical squared errors; predictive version relies sampling importance-resampling procedure. XX corresponds new set covariate values, set post_lpd = NULL yy = NULL (default values). Additional details predictive empirical comparisons pp_loss pp_loss_out.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family_binary.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the acceptable family for binary data — accept_family_binary","title":"Compute the acceptable family for binary data — accept_family_binary","text":"Given output Bayesian model candidate subsets, compute *acceptable family* subsets match nearly match predictive accuracy \"best\" subset. function applies binary data, logistic regression.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family_binary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the acceptable family for binary data — accept_family_binary","text":"","code":"accept_family_binary(   post_y_pred,   post_lpd,   XX,   indicators,   eps_level = 0.05,   eta_level = 0,   loss_type = \"cross-ent\",   yy = NULL,   post_y_hat = NULL,   K = 10,   sir_frac = 0.5,   plot = TRUE )"},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family_binary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the acceptable family for binary data — accept_family_binary","text":"post_y_pred S x n matrix posterior predictive given XX covariate values post_lpd S evaluations log-likelihood computed posterior draw parameters XX n x p matrix covariates evaluate indicators L x p matrix inclusion indicators (booleans) row denotes candidate subset eps_level probability required match predictive performance \"best\" model (eta_level) eta_level allowable margin ( \"best\" model loss_type loss function used: \"cross-ent\" (cross-entropy) \"misclass\" (misclassication rate) yy n-dimensional vector response variables post_y_hat S x n matrix posterior fitted values given XX covariate values (optional) K number cross-validation folds sir_frac fraction posterior samples use SIR plot logical; TRUE, include plot summarize predictive performance across candidate subsets","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family_binary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the acceptable family for binary data — accept_family_binary","text":"list containing following elements: all_accept: indices (.e., rows indicators) correspond acceptable subsets beta_hat_small linear coefficients smallest acceptable model beta_hat_min linear coefficients \"best\" acceptable model ell_small: index (.e., row indicators) smallest acceptable model ell_min: index (.e., row indicators) \"best\" acceptable model","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family_binary.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute the acceptable family for binary data — accept_family_binary","text":"see pp_loss_binary additional details predictive empirical comparisons.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family_randint.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the acceptable family of linear subsets for the\nrandom intercept model — accept_family_randint","title":"Compute the acceptable family of linear subsets for the\nrandom intercept model — accept_family_randint","text":"Given output Bayesian random intercept model candidate subsets, compute *acceptable family* subsets match nearly match predictive accuracy \"best\" subset. acceptable family may computed set covariate values XX; XX = X -sample points, cross-validation used assess --sample predictive performance.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family_randint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the acceptable family of linear subsets for the\nrandom intercept model — accept_family_randint","text":"","code":"accept_family_randint(   post_y_pred,   post_lpd,   post_sigma_e,   post_sigma_u,   XX,   YY,   indicators,   post_y_pred_sum = NULL,   eps_level = 0.05,   eta_level = 0,   K = 10,   sir_frac = 0.5,   plot = TRUE )"},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family_randint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the acceptable family of linear subsets for the\nrandom intercept model — accept_family_randint","text":"post_y_pred S x m x n matrix posterior predictive given XX covariate values m replicates per subject post_lpd S evaluations log-likelihood computed posterior draw parameters post_sigma_e (nsave) draws posterior distribution observation error SD post_sigma_u (nsave) draws posterior distribution random intercept SD XX n x p matrix covariates evaluate YY m x n matrix response variables (optional) indicators L x p matrix inclusion indicators (booleans) row denotes candidate subset post_y_pred_sum (nsave x n) matrix posterior predictive draws summed replicates within subject (optional) eps_level probability required match predictive performance \"best\" model (eta_level) eta_level allowable margin ( \"best\" model K number cross-validation folds (optional) sir_frac fraction posterior samples use SIR (optional) plot logical; TRUE, include plot summarize predictive performance across candidate subsets","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family_randint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the acceptable family of linear subsets for the\nrandom intercept model — accept_family_randint","text":"list containing following elements: all_accept: indices (.e., rows indicators) correspond acceptable subsets beta_hat_small linear coefficients smallest acceptable model beta_hat_min linear coefficients \"best\" acceptable model ell_small: index (.e., row indicators) smallest acceptable model ell_min: index (.e., row indicators) \"best\" acceptable model","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/accept_family_randint.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute the acceptable family of linear subsets for the\nrandom intercept model — accept_family_randint","text":"XX = X observed covariate values, post_lpd yy must provided. used compute cross-validated predictive empirical squared errors; predictive version relies sampling importance-resampling procedure. XX corresponds new set covariate values, set post_lpd = NULL yy = NULL (default values). Additional details predictive empirical comparisons pp_loss_randint.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/bayeslmm.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian linear mixed models (LMMs) — bayeslmm","title":"Bayesian linear mixed models (LMMs) — bayeslmm","text":"Efficient blocked Gibbs sampler Bayesian linear regression model random intercepts. fixed effects (regression coefficients) random effects (intercepts) sampled *jointly* Monte Carlo efficiency, variance components sampled separate block. model uses horseshoe prior fixed effects (regression coefficients).","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/bayeslmm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian linear mixed models (LMMs) — bayeslmm","text":"","code":"bayeslmm(Y, X, nsave = 1000, nburn = 1000)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/bayeslmm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian linear mixed models (LMMs) — bayeslmm","text":"Y (m x n) matrix response variables X (n x p) matrix covariates nsave number MCMC iterations record nburn number MCMC iterations discard (burn-)","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/bayeslmm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian linear mixed models (LMMs) — bayeslmm","text":"list containing following elements: coefficients estimated regression coefficients (posterior means) fitted.values fitted values (posterior means) post_y_pred posterior predictive draws post_y_pred_sum posterior predictive totals subject post_beta posterior draws regression coefficients post_u posterior draws random intercepts post_sigma_u posterior draws random intercept standard deviation post_sigma_e posterior draws observation error standard deviation post_lpd posterior draws log-likelihood (.e., log-likelihood evaluated posterior draw model parameters)","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/bayeslmm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian linear mixed models (LMMs) — bayeslmm","text":"","code":"# Simulate some data: dat = simulate_lm_randint(n = 100, # subjects                           p = 15,  # covariates                           m = 4)   # replicates per subject Y = dat$Y; X = dat$X  # Dimensions: dim(Y) # m x n #> [1]   4 100 dim(X) # n x p #> [1] 100  16  # Fit the model: fit = bayeslmm(Y = Y, X = X) # should take a few seconds names(fit) # what is returned #> [1] \"coefficients\"    \"fitted.values\"   \"post_y_pred\"     \"post_y_pred_sum\" #> [5] \"post_beta\"       \"post_u\"          \"post_sigma_u\"    \"post_sigma_e\"    #> [9] \"post_lpd\"         # Estimated coefficients: coef(fit) #>  [1] -0.86515934  0.68387359  0.26962628  0.91919177 -0.97129138 -1.02329040 #>  [7] -0.37538669  0.21228242  0.04044700 -0.09517231  0.23492811  0.08623728 #> [13]  0.20144228  0.07437913  0.12280698 -0.05407520  # Compare to ground truth: plot(coef(fit), dat$beta_true,      main = 'True and estimated coefficients',      xlab = 'Estimated', ylab = 'True') abline(0,1)   # 90% credible intervals: ci_beta = t(apply(fit$post_beta, 2,                   quantile, c(0.05, 0.95)))  # Fitted values (m x n): dim(fitted(fit)) #> [1]   4 100  # MCMC diagnostics: plot(as.ts(fit$post_beta[,1:6]))"},{"path":"https://drkowal.github.io/BayesSubsets/reference/branch_and_bound.html","id":null,"dir":"Reference","previous_headings":"","what":"Branch-and-bound algorithm for linear subset search — branch_and_bound","title":"Branch-and-bound algorithm for linear subset search — branch_and_bound","text":"Search \"best\" (according residual sum squares) linear subsets size. algorithm may collect n_best \"best\" subsets size, include exclude certain variables automatically, apply forward, backward, exhaustive search.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/branch_and_bound.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Branch-and-bound algorithm for linear subset search — branch_and_bound","text":"","code":"branch_and_bound(   yy,   XX,   wts = NULL,   n_best = 15,   to_include = 1,   to_exclude = NULL,   searchtype = \"exhaustive\" )"},{"path":"https://drkowal.github.io/BayesSubsets/reference/branch_and_bound.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Branch-and-bound algorithm for linear subset search — branch_and_bound","text":"yy vector response variables XX matrix covariates wts vector observation weights (weighted least squares) n_best number \"best\" subsets model size to_include indices covariates include ** subsets to_exclude indices covariates exclude ** subsets searchtype use exhaustive search, forward selection, backward selection sequential replacement search","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/branch_and_bound.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Branch-and-bound algorithm for linear subset search — branch_and_bound","text":"inclusion_index: matrix inclusion indicators (columns) subset returned (rows)","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/branch_and_bound.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Branch-and-bound algorithm for linear subset search — branch_and_bound","text":"","code":"# Simulate data: dat = simulate_lm(n = 100, p = 10)  # Run branch-and-bound: indicators = branch_and_bound(yy = dat$y, XX = dat$X)  # Inspect: head(indicators) #>            X1    X2    X3    X4    X5    X6    X7    X8    X9   X10   X11 #> force_in TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>          TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>          TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE #>          TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>          TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE #>          TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  # Dimensions: dim(indicators) #> [1] 127  11  # Model sizes: rowSums(indicators) #> force_in                                                                 #>        1        2        2        2        2        2        2        2  #>                                                                          #>        2        2        2        3        3        3        3        3  #>                                                                          #>        3        3        3        3        3        3        3        3  #>                                                                          #>        3        3        4        4        4        4        4        4  #>                                                                          #>        4        4        4        4        4        4        4        4  #>                                                                          #>        4        5        5        5        5        5        5        5  #>                                                                          #>        5        5        5        5        5        5        5        5  #>                                                                          #>        6        6        6        6        6        6        6        6  #>                                                                          #>        6        6        6        6        6        6        6        7  #>                                                                          #>        7        7        7        7        7        7        7        7  #>                                                                          #>        7        7        7        7        7        7        8        8  #>                                                                          #>        8        8        8        8        8        8        8        8  #>                                                                          #>        8        8        8        8        8        9        9        9  #>                                                                          #>        9        9        9        9        9        9        9        9  #>                                                                          #>        9        9        9        9       10       10       10       10  #>                                                                 #>       10       10       10       10       10       10       11"},{"path":"https://drkowal.github.io/BayesSubsets/reference/getXY_randint.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the pseudo X and Y variables for LMM summarization — getXY_randint","title":"Compute the pseudo X and Y variables for LMM summarization — getXY_randint","text":"Given output random intercept model, compute \"X\" \"Y\" variables needed least squares reparametrization.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/getXY_randint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the pseudo X and Y variables for LMM summarization — getXY_randint","text":"","code":"getXY_randint(   XX,   post_y_pred,   post_sigma_e,   post_sigma_u,   post_y_pred_sum = NULL )"},{"path":"https://drkowal.github.io/BayesSubsets/reference/getXY_randint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the pseudo X and Y variables for LMM summarization — getXY_randint","text":"XX (n x p) matrix covariates post_y_pred (nsave x m x n) array posterior predictive draws post_sigma_e (nsave) draws posterior distribution observation error SD post_sigma_u (nsave) draws posterior distribution random intercept SD post_y_pred_sum (nsave x n) matrix posterior predictive draws summed replicates within subject (optional)","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/getXY_randint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the pseudo X and Y variables for LMM summarization — getXY_randint","text":"list covariates response","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/initHS.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize the horseshoe prior parameters — initHS","title":"Initialize the horseshoe prior parameters — initHS","text":"Compute standard deviations, local global precisions, parameter expansion terms horseshoe prior initialization.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/initHS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize the horseshoe prior parameters — initHS","text":"","code":"initHS(omega)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/initHS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialize the horseshoe prior parameters — initHS","text":"omega n x p matrix evolution errors","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/initHS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initialize the horseshoe prior parameters — initHS","text":"List relevant components: sigma_wt, n x p matrix standard deviations, local global precisions parameter-expansion terms.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/lasso_path.html","id":null,"dir":"Reference","previous_headings":"","what":"(Adaptive) lasso for Bayesian variable selection — lasso_path","title":"(Adaptive) lasso for Bayesian variable selection — lasso_path","text":"Given output Bayesian model, compute (adaptive) lasso search path provide linear variable selection. function indicates variables selected along path.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/lasso_path.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"(Adaptive) lasso for Bayesian variable selection — lasso_path","text":"","code":"lasso_path(yy_hat, XX, wts = NULL, ww_hat = NULL)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/lasso_path.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"(Adaptive) lasso for Bayesian variable selection — lasso_path","text":"yy_hat vector fitted values XX matrix covariates wts vector observation weights (weighted least squares) ww_hat vector weights adaptive lasso","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/lasso_path.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"(Adaptive) lasso for Bayesian variable selection — lasso_path","text":"inclusion_index: matrix inclusion indicators (columns) subset returned (rows)","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/lasso_path.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"(Adaptive) lasso for Bayesian variable selection — lasso_path","text":"intercept XX matrix, assumed () intercept first column (ii) intercept included subset.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/loss_maha.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the pseudo X and Y variables for LMM summarization — loss_maha","title":"Compute the pseudo X and Y variables for LMM summarization — loss_maha","text":"Given output random intercept model, compute \"X\" \"Y\" variables needed least squares reparametrization.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/loss_maha.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the pseudo X and Y variables for LMM summarization — loss_maha","text":"","code":"loss_maha(YY, y_hat, m_scale)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/loss_maha.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the pseudo X and Y variables for LMM summarization — loss_maha","text":"YY m x n matrix response variables y_hat n x 1 vector fitted values (common across m replicates) m_scale Mahalanobis scale factor 1/(sigma_e^2/sigma_u^2 + m)","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/loss_maha.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the pseudo X and Y variables for LMM summarization — loss_maha","text":"Mahalanobis loss (scalar)","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/post_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Get posterior predictive draws and log-predictive density — post_predict","title":"Get posterior predictive draws and log-predictive density — post_predict","text":"Given posterior samples conditional mean conditional standard deviation Gaussian regression model, compute posterior predictive draws log-predictive density (lpd) observed data points (draw--draw).","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/post_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get posterior predictive draws and log-predictive density — post_predict","text":"","code":"post_predict(post_y_hat, post_sigma, yy = NULL)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/post_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get posterior predictive draws and log-predictive density — post_predict","text":"post_y_hat nsave x n draws conditional mean post_sigma nsave draws conditional standard deviation yy optional n-dimensional vector data points; NULL, lpd computed","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/post_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get posterior predictive draws and log-predictive density — post_predict","text":"list following elements: post_y_pred: nsave x n posterior predictive draws post_lpd: nsave x n evaluations log-predictive density","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/post_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get posterior predictive draws and log-predictive density — post_predict","text":"","code":"# Simulate data: dat = simulate_lm(n = 100, p = 10) y = dat$y; X = dat$X  # Fit a Bayesian linear model: fit = bayeslm::bayeslm(y ~ X[,-1], # intercept already included               N = 1000, burnin = 500) # small sim for ex #> horseshoe prior  #> fixed running time 0.00144501 #> sampling time 0.0207289  # Compute predictive draws: temp = post_predict(post_y_hat = tcrossprod(fit$beta, X),                     post_sigma = fit$sigma,                     yy = y) names(temp) # what is returned #> [1] \"post_y_pred\" \"post_lpd\"     # Compare fitted values to the truth: plot(dat$Ey_true,      colMeans(temp$post_y_pred),      xlab = 'True E(y | x)', ylab = 'Fitted') abline(0,1)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the predictive and empirical cross-validated squared error loss — pp_loss","title":"Compute the predictive and empirical cross-validated squared error loss — pp_loss","text":"Use posterior predictive draws sampling-importance resampling (SIR) algorithm approximate cross-validated predictive squared error loss. empirical squared error loss (.e., usual quantity cross-validation) also returned. values computed relative \"best\" subset according minimum empirical squared error loss. Specifically, quantities computed collection linear models fit Bayesian model output, linear model features different subset predictors.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the predictive and empirical cross-validated squared error loss — pp_loss","text":"","code":"pp_loss(   post_y_pred,   post_lpd,   XX,   yy,   indicators,   post_y_hat = NULL,   K = 10,   sir_frac = 0.5 )"},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the predictive and empirical cross-validated squared error loss — pp_loss","text":"post_y_pred S x n matrix posterior predictive given XX covariate values post_lpd S evaluations log-likelihood computed posterior draw parameters XX n x p matrix covariates evaluate yy n-dimensional vector response variables indicators L x p matrix inclusion indicators (booleans) row denotes candidate subset post_y_hat S x n matrix posterior fitted values given XX covariate values K number cross-validation folds sir_frac fraction posterior samples use SIR","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the predictive and empirical cross-validated squared error loss — pp_loss","text":"list two elements: pred_loss emp_loss predictive empirical loss, respectively, subset.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute the predictive and empirical cross-validated squared error loss — pp_loss","text":"quantity post_y_hat conditional expectation response covariate value (columns) using parameters sampled posterior (rows). Bayesian linear regression, term X %*% beta. unspecified, algorithm instead use post_y_pred, still correct lower Monte Carlo efficiency.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss_binary.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the predictive and empirical cross-validated loss for binary data. — pp_loss_binary","title":"Compute the predictive and empirical cross-validated loss for binary data. — pp_loss_binary","text":"Use posterior predictive draws sampling-importance resampling (SIR) algorithm approximate cross-validated predictive loss. empirical loss (.e., usual quantity cross-validation) also returned. values computed relative \"best\" subset according minimum empirical loss. Specifically, quantities computed collection linear models fit Bayesian model output, linear model features different subset predictors. loss function may chosen cross-entropy misclassification rate","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss_binary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the predictive and empirical cross-validated loss for binary data. — pp_loss_binary","text":"","code":"pp_loss_binary(   post_y_pred,   post_lpd,   XX,   yy,   indicators,   loss_type = \"cross-ent\",   post_y_hat = NULL,   K = 10,   sir_frac = 0.5 )"},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss_binary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the predictive and empirical cross-validated loss for binary data. — pp_loss_binary","text":"post_y_pred S x n matrix posterior predictive given XX covariate values post_lpd S evaluations log-likelihood computed posterior draw parameters XX n x p matrix covariates evaluate yy n-dimensional vector response variables indicators L x p matrix inclusion indicators (booleans) row denotes candidate subset loss_type loss function used: \"cross-ent\" (cross-entropy) \"misclass\" (misclassication rate) post_y_hat S x n matrix posterior fitted values given XX covariate values K number cross-validation folds sir_frac fraction posterior samples use SIR","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss_binary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the predictive and empirical cross-validated loss for binary data. — pp_loss_binary","text":"list two elements: pred_loss emp_loss predictive empirical loss, respectively, subset.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss_binary.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute the predictive and empirical cross-validated loss for binary data. — pp_loss_binary","text":"quantity post_y_hat conditional expectation response covariate value (columns) using parameters sampled posterior (rows). binary data, also estimated probability \"success\". unspecified, algorithm instead use post_y_pred, still correct lower Monte Carlo efficiency.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss_out.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the predictive squared error loss on *new* testing points — pp_loss_out","title":"Compute the predictive squared error loss on *new* testing points — pp_loss_out","text":"Use posterior predictive draws new XX points, compute predictive squared error loss. values computed relative largest subset provided, typically full set covariates (also minimizer expected predictive loss). quantities computed collection linear models fit Bayesian model output, linear model features different subset predictors.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss_out.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the predictive squared error loss on *new* testing points — pp_loss_out","text":"","code":"pp_loss_out(post_y_pred, XX, indicators, post_y_hat = NULL)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss_out.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the predictive squared error loss on *new* testing points — pp_loss_out","text":"post_y_pred S x n matrix posterior predictive given XX covariate values XX n x p matrix covariates evaluate indicators L x p matrix inclusion indicators (booleans) row denotes candidate subset post_y_hat S x n matrix posterior fitted values given XX covariate values","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss_out.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the predictive squared error loss on *new* testing points — pp_loss_out","text":"pred_loss: predictive loss subset.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss_out.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute the predictive squared error loss on *new* testing points — pp_loss_out","text":"quantity post_y_hat conditional expectation response covariate value (columns) using parameters sampled posterior (rows). Bayesian linear regression, term  X %*% beta. unspecified, algorithm instead use post_y_pred, still correct lower Monte Carlo efficiency.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss_randint.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the predictive and empirical cross-validated Mahalanobis loss\nunder the random intercept model — pp_loss_randint","title":"Compute the predictive and empirical cross-validated Mahalanobis loss\nunder the random intercept model — pp_loss_randint","text":"Use posterior predictive draws sampling-importance resampling (SIR) algorithm approximate cross-validated predictive Mahalanobis loss. empirical Mahalanobis loss also returned. values computed relative \"best\" subset according minimum empirical Mahalanobis loss. Specifically, quantities computed collection linear models fit Bayesian model output, linear model features different subset predictors.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss_randint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the predictive and empirical cross-validated Mahalanobis loss\nunder the random intercept model — pp_loss_randint","text":"","code":"pp_loss_randint(   post_y_pred,   post_lpd,   post_sigma_e,   post_sigma_u,   XX,   YY,   indicators,   post_y_pred_sum = NULL,   K = 10,   sir_frac = 0.5 )"},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss_randint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the predictive and empirical cross-validated Mahalanobis loss\nunder the random intercept model — pp_loss_randint","text":"post_y_pred S x m x n matrix posterior predictive given XX covariate values m replicates per subject post_lpd S evaluations log-likelihood computed posterior draw parameters post_sigma_e (nsave) draws posterior distribution observation error SD post_sigma_u (nsave) draws posterior distribution random intercept SD XX n x p matrix covariates evaluate YY m x n matrix response variables (optional) indicators L x p matrix inclusion indicators (booleans) row denotes candidate subset post_y_pred_sum (nsave x n) matrix posterior predictive draws summed replicates within subject (optional) K number cross-validation folds sir_frac fraction posterior samples use SIR","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/pp_loss_randint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the predictive and empirical cross-validated Mahalanobis loss\nunder the random intercept model — pp_loss_randint","text":"list two elements: pred_loss emp_loss predictive empirical loss, respectively, subset.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/prescreen.html","id":null,"dir":"Reference","previous_headings":"","what":"Marginal pre-screening algorithm — prescreen","title":"Marginal pre-screening algorithm — prescreen","text":"Given S draws p regression coefficients, function () computes (SD-standarized) posterior means, (ii) orders absolute values statistic, (iii) returns indices largest num_to_keep values.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/prescreen.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal pre-screening algorithm — prescreen","text":"","code":"prescreen(post_beta, num_to_keep)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/prescreen.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal pre-screening algorithm — prescreen","text":"post_beta (S x p) matrix posterior simulations p regression coefficients num_to_keep number variables return screening process","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/prescreen.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Marginal pre-screening algorithm — prescreen","text":"ind_keep: indices variables keep","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/prescreen.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Marginal pre-screening algorithm — prescreen","text":"Bayesian model nonlinear, post_beta may obtained instead projecting vector posterior predictive draws onto covariate matrix.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/prescreen_lasso.html","id":null,"dir":"Reference","previous_headings":"","what":"Marginal pre-screening algorithm given lasso output — prescreen_lasso","title":"Marginal pre-screening algorithm given lasso output — prescreen_lasso","text":"Given estimated lasso coefficients, return indices first num_to_keep variables enter model","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/prescreen_lasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal pre-screening algorithm given lasso output — prescreen_lasso","text":"","code":"prescreen_lasso(beta_hat_lasso, num_to_keep)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/prescreen_lasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal pre-screening algorithm given lasso output — prescreen_lasso","text":"beta_hat_lasso (p x L) matrix estimated coefficients lasso; sparsity decreases column index L num_to_keep number variables return screening process","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/prescreen_lasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Marginal pre-screening algorithm given lasso output — prescreen_lasso","text":"ind_keep: indices variables keep","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/proj_posterior.html","id":null,"dir":"Reference","previous_headings":"","what":"Projected predictive distribution for regression coefficients — proj_posterior","title":"Projected predictive distribution for regression coefficients — proj_posterior","text":"Given draws predictive distribution, project draws onto ( subset ) covariates. produces many predictive draws regression coefficients, provides uncertainty quantification.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/proj_posterior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Projected predictive distribution for regression coefficients — proj_posterior","text":"","code":"proj_posterior(post_y_pred, XX, sub_x = 1:ncol(XX), use_ols = TRUE)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/proj_posterior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Projected predictive distribution for regression coefficients — proj_posterior","text":"post_y_pred S x n matrix posterior predictive given XX covariate values XX n x p matrix covariates sub_x vector inclusion indicators p covariates; remaining coefficients fixed zero use_ols logical; TRUE, use ordinary least squares regression (default); otherwise use logistic regression","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/proj_posterior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Projected predictive distribution for regression coefficients — proj_posterior","text":"post_beta: S x p matrix draws projected predictive distribution  regression coefficients.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/proj_posterior_randint.html","id":null,"dir":"Reference","previous_headings":"","what":"Projected predictive distribution for regression coefficients\nin the random intercept model — proj_posterior_randint","title":"Projected predictive distribution for regression coefficients\nin the random intercept model — proj_posterior_randint","text":"Given draws predictive distribution random intercept model, project draws onto ( subset ) covariates using Mahalanobis loss. produces many predictive draws regression coefficients, provides uncertainty quantification.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/proj_posterior_randint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Projected predictive distribution for regression coefficients\nin the random intercept model — proj_posterior_randint","text":"","code":"proj_posterior_randint(   post_y_pred,   XX,   sub_x = 1:ncol(XX),   post_sigma_e,   post_sigma_u,   post_y_pred_sum = NULL )"},{"path":"https://drkowal.github.io/BayesSubsets/reference/proj_posterior_randint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Projected predictive distribution for regression coefficients\nin the random intercept model — proj_posterior_randint","text":"post_y_pred S x m x n matrix posterior predictive given XX covariate values XX n x p matrix covariates sub_x vector inclusion indicators p covariates; remaining coefficients fixed zero post_sigma_e (nsave) draws posterior distribution observation error SD post_sigma_u (nsave) draws posterior distribution random intercept SD post_y_pred_sum (nsave x n) matrix posterior predictive draws summed replicates within subject (optional)","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/proj_posterior_randint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Projected predictive distribution for regression coefficients\nin the random intercept model — proj_posterior_randint","text":"post_beta: S x p matrix draws projected predictive distribution  regression coefficients.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/sampleHS.html","id":null,"dir":"Reference","previous_headings":"","what":"Sampler for horseshoe prior parameters — sampleHS","title":"Sampler for horseshoe prior parameters — sampleHS","text":"Compute one draw horseshoe prior parameters (local precision, global precision, local global parameter expansion terms).","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/sampleHS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sampler for horseshoe prior parameters — sampleHS","text":"","code":"sampleHS(omega, params, sigma_e = 1)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/sampleHS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sampler for horseshoe prior parameters — sampleHS","text":"omega n x p matrix errors params list parameters update sigma_e observation error standard deviation; (optional) scaling purposes","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/sampleHS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sampler for horseshoe prior parameters — sampleHS","text":"List relevant components params: sigma_wt, n x p matrix standard deviations, local global precisions parameter-expansion terms.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/sampleHS.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Sampler for horseshoe prior parameters — sampleHS","text":"avoid scaling observation standard deviation sigma_e, simply use sigma_e = 1 functional call.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/simulate_lm.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate a Gaussian linear model — simulate_lm","title":"Simulate a Gaussian linear model — simulate_lm","text":"Generate data (sparse) Gaussian linear model. covariates correlated Gaussian variables. user may control signal--noise number nonzero coefficients.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/simulate_lm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate a Gaussian linear model — simulate_lm","text":"","code":"simulate_lm(n, p, p_sig = min(5, p/2), SNR = 1)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/simulate_lm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate a Gaussian linear model — simulate_lm","text":"n number observations p number covariates p_sig number true nonzero coefficients (signals) SNR signal--noise ratio","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/simulate_lm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate a Gaussian linear model — simulate_lm","text":"list following elements: y: response variable X: matrix covariates beta_true: true regression coefficients (including intercept) Ey_true: true expectation y (X%*%beta_true) sigma_true: true error standard deviation","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/simulate_lm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulate a Gaussian linear model — simulate_lm","text":"true regression coefficients include intercept (-1) otherwise p_sig nonzero coefficients half equal 1 half equal -1.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/simulate_lm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate a Gaussian linear model — simulate_lm","text":"","code":"# Simulate data: dat = simulate_lm(n = 100, p = 10) names(dat) # what is returned #> [1] \"y\"          \"X\"          \"beta_true\"  \"Ey_true\"    \"sigma_true\""},{"path":"https://drkowal.github.io/BayesSubsets/reference/simulate_lm_randint.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate a Gaussian linear model with random intercepts — simulate_lm_randint","title":"Simulate a Gaussian linear model with random intercepts — simulate_lm_randint","text":"Generate data (sparse) Gaussian linear model random intercepts, .e., repeated measurements (longitudinal) data. covariates correlated Gaussian variables. user may control signal--noise, number nonzero coefficients, intraclass correlation","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/simulate_lm_randint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate a Gaussian linear model with random intercepts — simulate_lm_randint","text":"","code":"simulate_lm_randint(n, p, m, rho = 0.25, p_sig = min(5, p/2), SNR = 1)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/simulate_lm_randint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate a Gaussian linear model with random intercepts — simulate_lm_randint","text":"n number subjects p number covariates m number observations per subject rho intraclass correlation coefficient p_sig number true nonzero coefficients (signals) SNR signal--noise ratio","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/simulate_lm_randint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate a Gaussian linear model with random intercepts — simulate_lm_randint","text":"list following elements: Y: matrix response variables X: matrix covariates beta_true: true regression coefficients (including intercept) Ey_true: true expectation y (X%*%beta_true) m_scale_true: true Mahalanobis scale factor, 1/(sigma_e^2/sigma_u^2 + m)","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/simulate_lm_randint.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulate a Gaussian linear model with random intercepts — simulate_lm_randint","text":"true regression coefficients include intercept (-1) otherwise p_sig nonzero coefficients half equal 1 half equal -1.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/simulate_lm_randint.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate a Gaussian linear model with random intercepts — simulate_lm_randint","text":"","code":"# Simulate data: dat = simulate_lm_randint(n = 100, p = 10, m = 4) names(dat) # what is returned #> [1] \"Y\"            \"X\"            \"beta_true\"    \"Ey_true\"      \"m_scale_true\""},{"path":"https://drkowal.github.io/BayesSubsets/reference/var_imp.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable importance for the acceptable family — var_imp","title":"Variable importance for the acceptable family — var_imp","text":"Given candidate subsets indicators acceptable family subsets, compute variable proportion acceptable subsets variable appears. specified, variable co-appearances can computed reported well.","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/var_imp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable importance for the acceptable family — var_imp","text":"","code":"var_imp(indicators, all_accept, co = TRUE, xnames = NULL)"},{"path":"https://drkowal.github.io/BayesSubsets/reference/var_imp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable importance for the acceptable family — var_imp","text":"indicators L x p matrix inclusion indicators (booleans) row denotes candidate subset all_accept indices (.e., rows indicators) correspond acceptable subsets co logical; TRUE, compute return co-variable importances xnames names x-variables","code":""},{"path":"https://drkowal.github.io/BayesSubsets/reference/var_imp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable importance for the acceptable family — var_imp","text":"list variable importances vi_inc co-variable importances vi_co","code":""}]
