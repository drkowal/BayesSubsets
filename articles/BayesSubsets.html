<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="BayesSubsets">
<title>Introduction to BayesSubsets • BayesSubsets</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Introduction to BayesSubsets">
<meta property="og:description" content="BayesSubsets">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">BayesSubsets</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item">
  <a class="nav-link" href="../articles/BayesSubsets.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/Binary-target.html">Targeted Prediction and Binary Outcomes</a>
    <a class="dropdown-item" href="../articles/High-dim.html">BayesSubsets with high-dimensional data</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/drkowal/BayesSubsets/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Introduction to BayesSubsets</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/drkowal/BayesSubsets/blob/HEAD/vignettes/BayesSubsets.Rmd" class="external-link"><code>vignettes/BayesSubsets.Rmd</code></a></small>
      <div class="d-none name"><code>BayesSubsets.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="overview">Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h2>
<p>Subset selection is a valuable tool for interpretable learning,
scientific discovery, and data compression. Given an outcome <span class="math inline">\(y\)</span> and <span class="math inline">\((n
\times p)\)</span> covariates <span class="math inline">\(X\)</span>,
the general goal is to find the subset of covariates (i.e., columns of
<span class="math inline">\(X\)</span>) that “best” predict <span class="math inline">\(y\)</span>, and to estimate the corresponding
regression coefficients.</p>
<p>In recent years, subset selection has declined in popularity for
several reasons:</p>
<ol style="list-style-type: decimal">
<li><p>Selection instability: it is very common to obtain completely
different “best” subsets under minor perturbations of the data;</p></li>
<li><p>Lack of regularization: once a “best” subset has been identified,
the coefficients are estimated using ordinary least squares (OLS)—which
is inadvisable when the subset is moderate/large and the covariates are
correlated, since it can induce variance-inflation; and</p></li>
<li><p>Computational bottlenecks: an exhaustive search must consider
<span class="math inline">\(2^p\)</span> subsets, which becomes
prohibitive for <span class="math inline">\(p &gt; 30\)</span>.</p></li>
</ol>
<p>As a result, frequentists have pursued penalized regression (e.g.,
the lasso), but also commonly select variables based on p-values (e.g.,
p-values <span class="math inline">\(&lt; 0.05\)</span>)—although this
is not typically referred to as “selection” explicitly. Similarly,
Bayesian methods predominantly consider <em>marginal</em> selection, for
example using posterior inclusion probabilities (e.g., from
spike-and-slab priors) or based on credible intervals that exclude zero.
However, marginal selection is unsatisfactory—we prefer to interpret any
subset as a <em>joint</em> collection of variables—and empirically
inferior, with substantially less power to detect true effects (Kowal <a href="https://doi.org/10.1080/01621459.2021.1891926" class="external-link">2021</a>, <a href="https://jmlr.org/papers/v23/21-0403.html" class="external-link">2022a</a>, <a href="https://doi.org/10.1111/biom.13707" class="external-link">2022b</a>).</p>
<p><code>BayesSubsets</code> seeks to reinvigorate (Bayesian) subset
selection. Given <em>any</em> Bayesian regression model <span class="math inline">\(M\)</span> that predicts <span class="math inline">\(y\)</span> from <span class="math inline">\(X\)</span>, we provide:</p>
<ul>
<li><p>Optimal <strong>linear</strong> coefficients for any subset of
variables, which produces a useful and interpretable summary of <span class="math inline">\(M\)</span> (which may be
complex/nonlinear);</p></li>
<li><p>Regularization (i.e., shrinkage) for these coefficients, which is
inherited from <span class="math inline">\(M\)</span> and helps guard
against variance-inflation;</p></li>
<li><p>Uncertainty quantification for these coefficients, again
leveraging <span class="math inline">\(M\)</span>;</p></li>
<li><p>The <strong>acceptable family</strong> of “near-optimal” subsets
of linear predictors that match or nearly match the “best” (by minimum
cross-validated error) subset; and</p></li>
<li><p>Summaries of the acceptable family, including the 1)
<strong>smallest acceptable subset</strong> and 2) a <strong>variable
importance</strong> metrics that computes, for each variable <span class="math inline">\(j\)</span>, the proportion of acceptable subsets
in which <span class="math inline">\(j\)</span> appears.</p></li>
</ul>
<p>In aggregate, these features address the limitations of (classical)
subset selection and provide a coherent Bayesian alternative anchored in
decision analysis.</p>
<p>A main contribution of <code>BayesSubsets</code> is the computation
and summarization of the acceptable family, which deemphasizes the role
of a single “best” subset and instead advances the broader perspective
that often many subsets are highly competitive. This is especially true
for datasets with correlated predictors, low signal-to-noise, and small
sample sizes. As a result, the acceptable family counteracts the
selection instability that has traditionally afflicted (classical)
subset selection.</p>
</div>
<div class="section level2">
<h2 id="using-bayessubsets">Using BayesSubsets<a class="anchor" aria-label="anchor" href="#using-bayessubsets"></a>
</h2>
<div class="section level3">
<h3 id="getting-started">Getting started<a class="anchor" aria-label="anchor" href="#getting-started"></a>
</h3>
<p>We begin by installing and loading the package:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># devtools::install_github("drkowal/BayesSubsets")</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/drkowal/BayesSubsets" class="external-link">BayesSubsets</a></span><span class="op">)</span></span></code></pre></div>
<p>For this example, we will consider simulated data with correlated
covariates <span class="math inline">\(X\)</span> and a continuous
outcome <span class="math inline">\(y \in \mathbb{R}\)</span>:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># To reproduce:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Simulate some data:</span></span>
<span><span class="va">dat</span> <span class="op">=</span> <span class="fu"><a href="../reference/simulate_lm.html">simulate_lm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">200</span>,   <span class="co"># number of observations</span></span>
<span>                  p <span class="op">=</span> <span class="fl">30</span>,    <span class="co"># number of predictors</span></span>
<span>                  p_sig <span class="op">=</span> <span class="fl">5</span>, <span class="co"># number of true signals</span></span>
<span>                  SNR <span class="op">=</span> <span class="fl">1</span>    <span class="co"># signal-to-noise ratio</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Store the data:</span></span>
<span><span class="va">y</span> <span class="op">=</span> <span class="va">dat</span><span class="op">$</span><span class="va">y</span>; <span class="va">X</span> <span class="op">=</span> <span class="va">dat</span><span class="op">$</span><span class="va">X</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="fitting-the-regression-model">Fitting the regression model<a class="anchor" aria-label="anchor" href="#fitting-the-regression-model"></a>
</h3>
<p>The first step is to fit a Bayesian regression model. Here, we will
use a linear model with horseshoe priors:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/JingyuHe/bayeslm" class="external-link">bayeslm</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit the Bayesian regression model:</span></span>
<span><span class="va">fit</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/bayeslm/man/bayeslm.html" class="external-link">bayeslm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">X</span><span class="op">[</span>,<span class="op">-</span><span class="fl">1</span><span class="op">]</span>, <span class="co"># intercept already included</span></span>
<span>              prior <span class="op">=</span> <span class="st">'horseshoe'</span>, <span class="co"># prior on regression coefficients</span></span>
<span>              N <span class="op">=</span> <span class="fl">10000</span>, <span class="co"># MCMC samples to save</span></span>
<span>              burnin <span class="op">=</span> <span class="fl">5000</span> <span class="co"># initial samples to discard</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; horseshoe prior </span></span>
<span><span class="co">#&gt; fixed running time 0.00444005</span></span>
<span><span class="co">#&gt; sampling time 0.804864</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="computing-optimal-linear-coefficients">Computing optimal linear coefficients<a class="anchor" aria-label="anchor" href="#computing-optimal-linear-coefficients"></a>
</h3>
<p>Given any Bayesian regression model <span class="math inline">\(M\)</span> and any subset of covariates <span class="math inline">\(S\)</span>, we compute the <strong>optimal linear
coefficients</strong> according to Bayesian decision analysis. <a href="https://doi.org/10.1080/01621459.2021.1891926" class="external-link">Kowal (2021)</a>
showed that this is obtained simply by projecting the fitted values
<span class="math inline">\(\hat y\)</span> from <span class="math inline">\(M\)</span> onto <span class="math inline">\(X_S\)</span>, i.e., the covariate matrix <span class="math inline">\(X\)</span> restricted to the columns selected in
<span class="math inline">\(S\)</span>. For an example subset <span class="math inline">\(S = \{1,3,10\}\)</span> and squared error loss,
the following code computes our optimal linear summary:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example subset:</span></span>
<span><span class="va">S_ex</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span>, <span class="fl">10</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Optimal coefficients:</span></span>
<span><span class="fu"><a href="../reference/get_coefs.html">get_coefs</a></span><span class="op">(</span>y_hat <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span>,</span>
<span>          XX <span class="op">=</span> <span class="va">X</span><span class="op">[</span>,<span class="va">S_ex</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt;         X1         X3        X10 </span></span>
<span><span class="co">#&gt; -0.8863355  0.4474228 -0.5731277</span></span></code></pre></div>
<p>We emphasize two key points, First, this is <em>not</em> the same as
classical linear regression with <span class="math inline">\(y\)</span>
and <span class="math inline">\(X\)</span>. To see this, recall that
under <span class="math inline">\(M\)</span>, the fitted values are
<span class="math inline">\(\hat y = X \hat \beta\)</span>, where <span class="math inline">\(\hat \beta\)</span> is the posterior expectation
of the regression coefficients from the Bayesian linear model. The
horseshoe prior on <span class="math inline">\(\beta\)</span> should
shrink many of these coefficients toward zero, which propagates
regularization to the optimal coefficients in the projection above.
Thus, our “fit-to-the-fit” resolves a key criticism of (classical)
subset selection.</p>
<p>Second, this is <em>not</em> a two-stage estimator in the classical
sense, where plug-in estimates from a stage-one model are plugged in for
an unknown parameter in a stage-two model. Instead, this is a fully
coherent decision analysis designed obtain a certain type of point
estimate—just like a posterior mean or a posterior median. For example,
the optimal coefficients for the full set of covariates <span class="math inline">\(S = \{1,\ldots,p\}\)</span> are simply <span class="math inline">\(\hat \beta\)</span>—the usual posterior mean under
<span class="math inline">\(M\)</span>. But these linear summaries are
more broad: they can be computed to summarize <em>any</em> Bayesian
regression model <span class="math inline">\(M\)</span> (not just linear
models) for <em>any</em> subset of covariates <span class="math inline">\(S\)</span>.</p>
</div>
<div class="section level3">
<h3 id="uncertainty-quantification-for-the-linear-coefficients">Uncertainty quantification for the linear coefficients<a class="anchor" aria-label="anchor" href="#uncertainty-quantification-for-the-linear-coefficients"></a>
</h3>
<p>We may also obtain posterior uncertainty quantification for the
linear coefficients that are active (nonzero) in <span class="math inline">\(S\)</span>. To do so, we project the posterior
predictive distribution onto <span class="math inline">\(X_S\)</span>
draw-by-draw, which induces a posterior predictive distribution for the
linear coefficients under the model <span class="math inline">\(M\)</span>—even though <span class="math inline">\(M\)</span> need not be linear in general.</p>
<p>These predictive draws are not automatically output by
<code>bayeslm</code>, so we run the following code to sample them. We
also compute the log-predictive densities which will later be used in
predictive cross-validation.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Extract the posterior predictive draws and lpd:</span></span>
<span><span class="va">temp</span> <span class="op">=</span> <span class="fu"><a href="../reference/post_predict.html">post_predict</a></span><span class="op">(</span>post_y_hat <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html" class="external-link">tcrossprod</a></span><span class="op">(</span><span class="va">fit</span><span class="op">$</span><span class="va">beta</span>, <span class="va">X</span><span class="op">)</span>,</span>
<span>                    post_sigma <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">sigma</span>,</span>
<span>                    yy <span class="op">=</span> <span class="va">y</span><span class="op">)</span></span>
<span><span class="va">post_y_pred</span> <span class="op">=</span> <span class="va">temp</span><span class="op">$</span><span class="va">post_y_pred</span></span>
<span><span class="va">post_lpd</span> <span class="op">=</span> <span class="va">temp</span><span class="op">$</span><span class="va">post_lpd</span></span></code></pre></div>
<p>Now, we can obtain posterior predictive samples of the linear
coefficients in <span class="math inline">\(S\)</span>, and summarize
those posteriors using 95% credible intervals.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Posterior predictive draws of *all* coefficients:</span></span>
<span><span class="va">post_beta_s</span> <span class="op">=</span> <span class="fu"><a href="../reference/proj_posterior.html">proj_posterior</a></span><span class="op">(</span>post_y_pred <span class="op">=</span> <span class="va">post_y_pred</span>,</span>
<span>                                 XX <span class="op">=</span> <span class="va">X</span>,</span>
<span>                                 sub_x <span class="op">=</span> <span class="va">S_ex</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">post_beta_s</span><span class="op">)</span> <span class="co"># the coefficients outside S_ex are fixed at zero</span></span>
<span><span class="co">#&gt; [1] 10000    31</span></span>
<span></span>
<span><span class="co"># Compute 95% credible intervals for the nonzero entries:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html" class="external-link">apply</a></span><span class="op">(</span><span class="va">post_beta_s</span><span class="op">[</span>,<span class="va">S_ex</span><span class="op">]</span>, <span class="fl">2</span>, </span>
<span>        <span class="va">quantile</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.05</span><span class="op">/</span><span class="fl">2</span>, <span class="fl">1</span> <span class="op">-</span> <span class="fl">0.05</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;            2.5%      97.5%</span></span>
<span><span class="co">#&gt; X1  -1.30024730 -0.4645863</span></span>
<span><span class="co">#&gt; X3  -0.01006303  0.9018901</span></span>
<span><span class="co">#&gt; X10 -1.00739359 -0.1385361</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="bayesian-subset-search">Bayesian subset search<a class="anchor" aria-label="anchor" href="#bayesian-subset-search"></a>
</h3>
<p>To this point, we have focused on point and interval (linear)
summaries for an arbitrary yet fixed subset <span class="math inline">\(S\)</span>. However, we are often interested in
<em>searching</em> across subsets and measuring the predictive
performances. Here, we use the model <span class="math inline">\(M\)</span> output to generate a collection of
“candidate subsets” using decision analysis <a href="https://jmlr.org/papers/v23/21-0403.html" class="external-link">(Kowal, 2022a)</a>. For
small <span class="math inline">\(p\)</span> it may be possible to
enumerate all possible subsets. Here, we screen to the “best”
<code>n_best = 50</code> models of each size according to squared error
loss. We store these in a Boolean matrix <code>indicators</code>: each
row is an individual subset, while the columns indicate which variables
are included (<code>TRUE</code>) or excluded (<code>FALSE</code>).</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">indicators</span> <span class="op">=</span> <span class="fu"><a href="../reference/branch_and_bound.html">branch_and_bound</a></span><span class="op">(</span>yy <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span>, <span class="co"># response is the fitted values</span></span>
<span>                             XX <span class="op">=</span> <span class="va">X</span>,            <span class="co"># covariates</span></span>
<span>                             n_best <span class="op">=</span> <span class="fl">50</span>        <span class="co"># restrict to the "best" 50 subsets of each size</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Inspect:</span></span>
<span><span class="va">indicators</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span></span>
<span><span class="co">#&gt;            X1    X2    X3    X4    X5    X6    X7    X8    X9   X10</span></span>
<span><span class="co">#&gt; force_in TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span><span class="co">#&gt;          TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span><span class="co">#&gt;          TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span><span class="co">#&gt;          TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span><span class="co">#&gt;          TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span></span>
<span><span class="co"># Dimensions:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">indicators</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1412   31</span></span>
<span></span>
<span><span class="co"># Summarize the model sizes:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html" class="external-link">table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colSums.html" class="external-link">rowSums</a></span><span class="op">(</span><span class="va">indicators</span><span class="op">)</span><span class="op">)</span> <span class="co"># note: intercept always included</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 </span></span>
<span><span class="co">#&gt;  1 30 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 </span></span>
<span><span class="co">#&gt; 27 28 29 30 31 </span></span>
<span><span class="co">#&gt; 50 50 50 30  1</span></span></code></pre></div>
<p>When <span class="math inline">\(p \gg 30\)</span>, it is recommended
to use the <code>prescreen</code> function, which restricts the search
to include only <code>num_to_keep</code> possible active variables. This
makes the branch-and-bound algorithm feasible for very large <span class="math inline">\(p\)</span>.</p>
</div>
<div class="section level3">
<h3 id="the-acceptable-family-of-near-optimal-subsets">The acceptable family of “near-optimal” subsets<a class="anchor" aria-label="anchor" href="#the-acceptable-family-of-near-optimal-subsets"></a>
</h3>
<p>From this large collection of 1412 candidate subsets, we seek to
filter to the <strong>acceptable family</strong> of subsets, i.e., those
“near-optimal” subsets that predict about as well as the “best” subset.
These are computed based on 10-fold cross-validation, and use the
out-of-sample predictive distribution from <span class="math inline">\(M\)</span> to provide uncertainty quantification
for predictive accuracy.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute the acceptable family:</span></span>
<span><span class="va">accept_info</span> <span class="op">=</span> <span class="fu"><a href="../reference/accept_family.html">accept_family</a></span><span class="op">(</span>post_y_pred <span class="op">=</span> <span class="va">post_y_pred</span>,</span>
<span>                            post_lpd <span class="op">=</span> <span class="va">post_lpd</span>,</span>
<span>                            XX <span class="op">=</span> <span class="va">X</span>,</span>
<span>                            indicators <span class="op">=</span> <span class="va">indicators</span>,</span>
<span>                            yy <span class="op">=</span> <span class="va">y</span>,</span>
<span>                            post_y_hat <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/crossprod.html" class="external-link">tcrossprod</a></span><span class="op">(</span><span class="va">fit</span><span class="op">$</span><span class="va">beta</span>, <span class="va">X</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="BayesSubsets_files/figure-html/accept-1.png" width="576"></p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># How many subsets are in the acceptable family?</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">accept_info</span><span class="op">$</span><span class="va">all_accept</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1163</span></span>
<span></span>
<span><span class="co"># These are the rows of `indicators` that belong to the acceptable family:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">accept_info</span><span class="op">$</span><span class="va">all_accept</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 182 232 233 234 235 236</span></span>
<span></span>
<span><span class="co"># An example acceptable subset:</span></span>
<span><span class="va">ex_accept</span> <span class="op">=</span> <span class="va">accept_info</span><span class="op">$</span><span class="va">all_accept</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/which.html" class="external-link">which</a></span><span class="op">(</span><span class="va">indicators</span><span class="op">[</span><span class="va">ex_accept</span>,<span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt; X1 X2 X3 X4 X5 X6 </span></span>
<span><span class="co">#&gt;  1  2  3  4  5  6</span></span></code></pre></div>
<p>The plot shows how the out-of-sample predictive performance varies
across subsets of different sizes, specifically relative (% change) to
the “best” subset (by minimum cross-validated error; dashed gray
vertical line). The x-marks are the (usual) empirical cross-validated
error, while the intervals leverage the predictive distribution from
<span class="math inline">\(M\)</span> to quantify uncertainty in the
out-of-sample predictive performance. While performance improves as
variables are added, it is clear that several smaller subsets are highly
competitive—especially when accounting for the predictive
uncertainty.</p>
</div>
<div class="section level3">
<h3 id="subset-selection-the-smallest-acceptable-subset">Subset selection: the smallest acceptable subset<a class="anchor" aria-label="anchor" href="#subset-selection-the-smallest-acceptable-subset"></a>
</h3>
<p>If we wish to <strong>select</strong> a single subset, a compelling
representative of the acceptable family is the <strong>smallest</strong>
acceptable subset. This choice favors parsimony, while its membership in
the acceptable family implies that it meets a high standard for
predictive accuracy. From the previous plot, we select the smallest
subset for which the intervals include zero (solid gray vertical
line).</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simplest acceptable subset:</span></span>
<span><span class="va">beta_hat_small</span> <span class="op">=</span> <span class="va">accept_info</span><span class="op">$</span><span class="va">beta_hat_small</span></span>
<span></span>
<span><span class="co"># Which coefficients are nonzero:</span></span>
<span><span class="va">S_small</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html" class="external-link">which</a></span><span class="op">(</span><span class="va">beta_hat_small</span> <span class="op">!=</span> <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># How many coefficients are nonzero:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">S_small</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 6</span></span></code></pre></div>
<p>The “best” subset by minimum cross-validation often includes many
extraneous variables, which is a well-known (and undesirable) byproduct
of cross-validation.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Acceptable subset that minimizes CV error:</span></span>
<span><span class="va">beta_hat_min</span> <span class="op">=</span> <span class="va">accept_info</span><span class="op">$</span><span class="va">beta_hat_min</span></span>
<span></span>
<span><span class="co"># Typically much larger (and often too large...)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">beta_hat_min</span> <span class="op">!=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 19</span></span></code></pre></div>
<p>For reference, the true model size is 6. Clearly, the “best” subset
is unsatisfactory.</p>
<p>Returning to the <em>smallest</em> acceptable subset, we can obtain
posterior samples and credible intervals for the coefficients as
before:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Draws from the posterior predictive distribution</span></span>
<span><span class="va">post_beta_small</span> <span class="op">=</span> <span class="fu"><a href="../reference/proj_posterior.html">proj_posterior</a></span><span class="op">(</span>post_y_pred <span class="op">=</span> <span class="va">post_y_pred</span>,</span>
<span>                                 XX <span class="op">=</span> <span class="va">X</span>,</span>
<span>                                 sub_x <span class="op">=</span> <span class="va">S_small</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute 95% credible intervals for the nonzero entries:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html" class="external-link">apply</a></span><span class="op">(</span><span class="va">post_beta_small</span><span class="op">[</span>,<span class="va">S_small</span><span class="op">]</span>, <span class="fl">2</span>, </span>
<span>        <span class="va">quantile</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.05</span><span class="op">/</span><span class="fl">2</span>, <span class="fl">1</span> <span class="op">-</span> <span class="fl">0.05</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;          2.5%      97.5%</span></span>
<span><span class="co">#&gt; X1 -1.4434868 -0.6028323</span></span>
<span><span class="co">#&gt; X2  0.3398519  1.6808732</span></span>
<span><span class="co">#&gt; X3  0.3738225  1.3484581</span></span>
<span><span class="co">#&gt; X4  0.1798377  1.5472092</span></span>
<span><span class="co">#&gt; X5 -1.6691666 -0.5500480</span></span>
<span><span class="co">#&gt; X6 -1.4600657 -0.2625247</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="variable-importance-from-acceptable-subsets">Variable importance from acceptable subsets<a class="anchor" aria-label="anchor" href="#variable-importance-from-acceptable-subsets"></a>
</h3>
<p>Another useful summary of the acceptable family is the
<strong>variable importance</strong>, which reports, for each variable
<span class="math inline">\(j\)</span>, the proportion of acceptable
subsets in which <span class="math inline">\(j\)</span> appears. We are
particularly interested in distinguishing among those variables that
occur in <em>all</em>, <em>some</em>, or <em>no</em> acceptable subsets,
which provides insight about which variables are indispensable
(“keystone covariates”) and which variables are part of a “predictively
plausible” explanation.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Variable importance: proportion of *acceptable subsets* in which each variable appears</span></span>
<span><span class="va">vi_e</span> <span class="op">=</span> <span class="fu"><a href="../reference/var_imp.html">var_imp</a></span><span class="op">(</span>indicators <span class="op">=</span> <span class="va">indicators</span>,</span>
<span>               all_accept <span class="op">=</span> <span class="va">accept_info</span><span class="op">$</span><span class="va">all_accept</span><span class="op">)</span><span class="op">$</span><span class="va">vi_inc</span></span>
<span></span>
<span><span class="co"># "Keystone covariates" that appear in *all* acceptable families:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/which.html" class="external-link">which</a></span><span class="op">(</span><span class="va">vi_e</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; 1 4 </span></span>
<span><span class="co">#&gt; 1 4</span></span>
<span></span>
<span><span class="co"># Irrelevant covariates that appear in *no* acceptable families:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/which.html" class="external-link">which</a></span><span class="op">(</span><span class="va">vi_e</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> </span>
<span><span class="co">#&gt; named integer(0)</span></span>
<span></span>
<span><span class="co"># Visualize:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/barplot.html" class="external-link">barplot</a></span><span class="op">(</span><span class="va">vi_e</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/order.html" class="external-link">order</a></span><span class="op">(</span><span class="va">vi_e</span>, <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">ncol</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">:</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">]</span>, <span class="co"># order...</span></span>
<span>        horiz <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>        main <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">'Variable importance for the acceptable family'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html" class="external-link">abline</a></span><span class="op">(</span>v <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p><img src="BayesSubsets_files/figure-html/vi-1.png" width="576"></p>
<p>We can see that quite a few variables appear in <em>almost</em> all
acceptable subsets: there are 6 variables that belong to at least 95% of
the acceptable subsets. There are also 6 variables that belong to fewer
than 20% of the acceptable subsets. Note that the covariates are highly
correlated in this simulated example (and <span class="math inline">\(p\)</span> is moderate), so it is reasonable to
expect that many covariates are roughly interchangeable in terms of
predictive accuracy.</p>
</div>
<div class="section level3">
<h3 id="comparing-with-traditional-posterior-summaries">Comparing with traditional posterior summaries<a class="anchor" aria-label="anchor" href="#comparing-with-traditional-posterior-summaries"></a>
</h3>
<p>Typically, Bayesian linear regression would report the posterior
expectations and 95% posterior credible intervals of the regression
coefficients <span class="math inline">\(\beta\)</span>. We plot these
together with the point and interval estimates for the <em>smallest</em>
acceptable model:</p>
<p><img src="BayesSubsets_files/figure-html/plot-1.png" width="576"></p>
<p>The traditional model summaries are completely dense: the point
estimates <span class="math inline">\(\hat \beta\)</span> are nonzero
for <em>all</em> covariates. By comparison, the point estimates from the
smallest acceptable subset are sparse, with only 6 active coefficients.
By design, the smallest acceptable subset only reports interval
estimates for these active coefficients. In this example, the intervals
are narrower for the smallest acceptable subset, while the traditional
intervals produce large intervals even for the truly zero
coefficients.</p>
<p>Finally, we verify the predictive accuracy to recover the true
regression surface, <span class="math inline">\(X\beta_{true}\)</span>:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># RMSE from posterior mean:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">dat</span><span class="op">$</span><span class="va">Ey_true</span> <span class="op">-</span> <span class="va">X</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="va">beta_hat</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.5382842</span></span>
<span></span>
<span><span class="co"># RMSE from smallest acceptable subset:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">dat</span><span class="op">$</span><span class="va">Ey_true</span> <span class="op">-</span> <span class="va">X</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="va">beta_hat_small</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.1924557</span></span></code></pre></div>
<p>Here, the smallest acceptable subset is an exceptional predictor.
However, keep in mind that it is optimized for <em>near-optimal</em>
predictive accuracy along with <em>simplicity</em> (sparsity). This is
often a potent combination, but we make no claims that it will uniformly
outperform other members of the acceptable family.</p>
</div>
<div class="section level3">
<h3 id="conclusion">Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"></a>
</h3>
<p>We have sought to demonstrate that <code>BayesSubsets</code> is a
useful accompaniment to the usual Bayesian regression workflow. Given
any Bayesian regression model <span class="math inline">\(M\)</span>, we
have shown how to compute</p>
<ol style="list-style-type: decimal">
<li><p>Optimal linear summaries for any subset of covariates;</p></li>
<li><p>Accompanying uncertainty quantification via posterior
(predictive) distributions and intervals;</p></li>
<li><p>Bayesian subset search using decision analysis;</p></li>
<li><p>The acceptable family of near-optimal subsets; and</p></li>
<li><p>Key summaries of the acceptable family, including the smallest
acceptable subset and a variable importance metric.</p></li>
</ol>
<p>When a single subset is required and parsimony is valued, then we
recommend the <strong>smallest acceptable subset</strong>. However, we
caution against the overreliance on any single subset without compelling
motivation. A key contribution of the acceptable family is that it
identifies <em>many</em> competing explanations (subsets) that are
nearly indistinguishable in predictive accuracy. From a purely
predictive perspective, we cannot completely rule out any member of the
acceptable family. Thus, we further recommend reporting the
<strong>variable importance</strong> as a default, variable-specific
summary of the acceptable family.</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Dan Kowal.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
