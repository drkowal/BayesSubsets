<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="BayesSubsets">
<title>Bayesian subset selection for linear mixed models • BayesSubsets</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Bayesian subset selection for linear mixed models">
<meta property="og:description" content="BayesSubsets">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">BayesSubsets</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/BayesSubsets.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/Binary-target.html">Targeted Prediction and Binary Outcomes</a>
    <a class="dropdown-item" href="../articles/High-dim.html">BayesSubsets with high-dimensional data</a>
    <a class="dropdown-item" href="../articles/Linear-mixed.html">Bayesian subset selection for linear mixed models</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/drkowal/BayesSubsets/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Bayesian subset selection for linear mixed models</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/drkowal/BayesSubsets/blob/HEAD/vignettes/Linear-mixed.Rmd" class="external-link"><code>vignettes/Linear-mixed.Rmd</code></a></small>
      <div class="d-none name"><code>Linear-mixed.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="linear-mixed-models">Linear Mixed Models<a class="anchor" aria-label="anchor" href="#linear-mixed-models"></a>
</h2>
<p>Linear mixed models (LMMs) are instrumental for regression analysis
with structured dependence, such as grouped, clustered, or multilevel
data. However, selection among the covariates—while accounting for this
structured dependence—remains a challenge.</p>
<p>Here, we explore the Bayesian subset selection strategies for LMMs
from <a href="https://doi.org/10.1111/biom.13707" class="external-link">Kowal (2022)</a>. A
central priority is to account for the <em>structured dependence</em>
modeled by the LMM, both in the selection criteria and the performance
metrics. Thus, our decision analysis uses <strong>Mahalanobis
loss</strong> to measure accuracy. The weighting matrix in the
Mahalanobis loss is the inverse (marginal) covariance matrix in the LMM.
This quantity depends on the variances of the random effects and the
errors, and thus requires careful attention for extracting optimal
coefficients, providing proper uncertainty quantification, and designing
scalable computations.</p>
<div class="section level3">
<h3 id="random-intercept-model">Random intercept model<a class="anchor" aria-label="anchor" href="#random-intercept-model"></a>
</h3>
<p>Suppose we observe repeated measurements <span class="math inline">\(y_{ij}\)</span>, <span class="math inline">\(j=1,\ldots,m_i\)</span> along with <span class="math inline">\(p\)</span> covariates <span class="math inline">\(x_i\)</span> for each individual <span class="math inline">\(i=1,\ldots,n\)</span>. The <em>random intercept
model</em> is a LMM that seeks to capture the within-subject dependence:
<span class="math display">\[
y_{ij} = x_i'\beta + u_i + \epsilon_{ij}
\]</span> where <span class="math inline">\(u_i \sim
N(0,\sigma_u^2)\)</span> and <span class="math inline">\(\epsilon_{ij}
\sim N(0, \sigma_\epsilon^2)\)</span> are mutually independent. By
design, the random intercept <span class="math inline">\(u_i\)</span> is
shared among all replicates for subject <span class="math inline">\(i\)</span> and induces the within-subject
correlation <span class="math display">\[
corr(y_{ij}, y_{ij'}) = \sigma_u^2/(\sigma_u^2 + \sigma_\epsilon^2)
\]</span> conditional on <span class="math inline">\(x_i\)</span> and
<span class="math inline">\(\beta\)</span>.</p>
<p>The <strong>goal</strong> is to perform subset selection for <span class="math inline">\(x\)</span> while accounting for these
intra-subject dependencies. We do so using Mahalanobis loss, which
modifies squared error loss to include a weight matrix <span class="math inline">\(\Omega\)</span>: <span class="math display">\[
\Vert v \Vert_\Omega^2 = v'\Omega v
\]</span> For the random intercept model, the (scaled) Mahalanobis
weight matrix is <span class="math display">\[
\sigma_\epsilon^2 \Omega = \mbox{bdiag}\{I_{m_i} -
(\sigma_\epsilon^2/\sigma_u^2 + m_i)^{-1} 1_{m_i}1_{m_i}'
\}
\]</span> where bdiag constructs a block-diagonal matrix across <span class="math inline">\(i=1,\ldots,n\)</span>, <span class="math inline">\(I_m\)</span> is the <span class="math inline">\(m\times m\)</span> identity, and <span class="math inline">\(1_m\)</span> is the <span class="math inline">\(m\times 1\)</span> vector of ones. Clearly, this
term depends on model parameters <span class="math inline">\((\sigma_\epsilon, \sigma_u)\)</span> and requires
careful consideration for efficient computing.</p>
</div>
<div class="section level3">
<h3 id="computing-for-the-random-intercept-model">Computing for the random intercept model<a class="anchor" aria-label="anchor" href="#computing-for-the-random-intercept-model"></a>
</h3>
<p><code>BayesSubsets</code> includes a highly efficient MCMC sampler
for the random intercept model via the function <code><a href="../reference/bayeslmm.html">bayeslmm()</a></code>.
In particular, our algorithm <em>jointly</em> samples the fixed and
random effects <span class="math inline">\((\beta, \{u_i\})\)</span>
using fast and scalable steps. This joint sampler avoids the
inefficiencies that arise from iteratively sampling full conditionals of
<span class="math inline">\(\beta\)</span> given <span class="math inline">\(u\)</span> and vice versa—which can be so
debilitating that it often requires alternative parametrizations (e.g.,
centered vs. noncentered) or sampling strategies (e.g., interweaving or
Stan). Our joint sampler eliminates these issues entirely, while
crucially maintaining computational scalability.</p>
</div>
<div class="section level3">
<h3 id="subset-selection">Subset selection<a class="anchor" aria-label="anchor" href="#subset-selection"></a>
</h3>
<p>Like the other methods implemented in <code>BayesSubsets</code>, we
provide</p>
<ul>
<li><p>Optimal <strong>linear</strong> coefficients for any subset of
variables;</p></li>
<li><p>Regularization (i.e., shrinkage) for these coefficients, which is
inherited from the LMM and helps guard against
variance-inflation;</p></li>
<li><p>Uncertainty quantification for these coefficients, again
leveraging the LMM;</p></li>
<li><p>The <strong>acceptable family</strong> of “near-optimal” subsets
of linear predictors that match or nearly match the “best” (by minimum
cross-validated error) subset; and</p></li>
<li><p>Summaries of the acceptable family, including the 1)
<strong>smallest acceptable subset</strong> and 2) a <strong>variable
importance</strong> metrics that computes, for each variable <span class="math inline">\(j\)</span>, the proportion of acceptable subsets
in which <span class="math inline">\(j\)</span> appears.</p></li>
</ul>
<p>The <strong>key difference</strong> from the alternative subset
selection strategies (e.g., Kowal <a href="https://doi.org/10.1080/01621459.2021.1891926" class="external-link">2021</a>, <a href="https://jmlr.org/papers/v23/21-0403.html" class="external-link">2022a</a>) is that each
of the above terms uses Mahalanobis loss to account for the structured
dependence modeled by the LMM.</p>
</div>
</div>
<div class="section level2">
<h2 id="getting-started">Getting started<a class="anchor" aria-label="anchor" href="#getting-started"></a>
</h2>
<p>We begin by installing and loading the package:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># devtools::install_github("drkowal/BayesSubsets")</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/drkowal/BayesSubsets" class="external-link">BayesSubsets</a></span><span class="op">)</span></span></code></pre></div>
<p>For this example, we will consider simulated data with <span class="math inline">\(n \times p\)</span> correlated covariates <span class="math inline">\(X\)</span> and <span class="math inline">\(m
\times n\)</span> response matrix <span class="math inline">\(Y\)</span>, where <span class="math inline">\(m\)</span> is the number of replicates per
subject.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># To reproduce:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Simulate some data:</span></span>
<span><span class="va">dat</span> <span class="op">=</span> <span class="fu"><a href="../reference/simulate_lm_randint.html">simulate_lm_randint</a></span><span class="op">(</span></span>
<span>  n <span class="op">=</span> <span class="fl">200</span>,   <span class="co"># number of subjects</span></span>
<span>  p <span class="op">=</span> <span class="fl">10</span>,    <span class="co"># number of predictors</span></span>
<span>  m <span class="op">=</span> <span class="fl">5</span>,     <span class="co"># number of replicates per subject</span></span>
<span>  p_sig <span class="op">=</span> <span class="fl">5</span>, <span class="co"># number of true signals</span></span>
<span>  SNR <span class="op">=</span> <span class="fl">1</span>    <span class="co"># signal-to-noise ratio</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Store the data:</span></span>
<span><span class="va">Y</span> <span class="op">=</span> <span class="va">dat</span><span class="op">$</span><span class="va">Y</span>; <span class="va">X</span> <span class="op">=</span> <span class="va">dat</span><span class="op">$</span><span class="va">X</span></span></code></pre></div>
<p>Next, we fit a Bayesian LMM with random intercepts. Anticipating
sparsity in the regression coefficients, we specify a horseshoe prior
for <span class="math inline">\(\beta\)</span>. The following function
fits this model using a highly efficient MCMC sampling algorithm:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit the Bayesian linear mixed model:</span></span>
<span><span class="va">fit</span> <span class="op">=</span> <span class="fu"><a href="../reference/bayeslmm.html">bayeslmm</a></span><span class="op">(</span>Y <span class="op">=</span> <span class="va">Y</span>,</span>
<span>               X <span class="op">=</span> <span class="va">X</span>,</span>
<span>               nsave <span class="op">=</span> <span class="fl">1000</span>, <span class="co"># MCMC samples to save</span></span>
<span>               nburn <span class="op">=</span> <span class="fl">1000</span> <span class="co"># initial samples to discard</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>For any subset of covariates <span class="math inline">\(S\)</span>,
we compute the <strong>optimal linear coefficients</strong> according to
Bayesian decision analysis <a href="https://doi.org/10.1111/biom.13707" class="external-link">(Kowal, 2022)</a>. For an
example subset <span class="math inline">\(S = \{1,3,10\}\)</span> and
Mahalanobis loss, the following code computes our optimal linear summary
under the random intercept model:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example subset:</span></span>
<span><span class="va">S_ex</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span>, <span class="fl">10</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Optimal coefficients:</span></span>
<span><span class="fu"><a href="../reference/get_coefs_randint.html">get_coefs_randint</a></span><span class="op">(</span></span>
<span>  post_y_pred <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_y_pred</span>,</span>
<span>  XX <span class="op">=</span> <span class="va">X</span><span class="op">[</span>,<span class="va">S_ex</span><span class="op">]</span>,</span>
<span>  post_sigma_e <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_sigma_e</span>,</span>
<span>  post_sigma_u <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_sigma_u</span>,</span>
<span>  post_y_pred_sum <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_y_pred_sum</span><span class="op">)</span></span>
<span><span class="co">#&gt;         X1         X3        X10 </span></span>
<span><span class="co">#&gt; -1.2310866  1.9174218 -0.5300475</span></span>
<span></span>
<span><span class="co"># Compare to the posterior mean for these coefficients:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="va">S_ex</span><span class="op">]</span></span>
<span><span class="co">#&gt; [1] -1.18108805  1.03223725  0.02845451</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="uncertainty-quantification-for-the-linear-coefficients">Uncertainty quantification for the linear coefficients<a class="anchor" aria-label="anchor" href="#uncertainty-quantification-for-the-linear-coefficients"></a>
</h2>
<p>We may also obtain posterior uncertainty quantification for the
linear coefficients that are active (nonzero) in <span class="math inline">\(S\)</span>. To do so, we project the posterior
predictive distribution onto <span class="math inline">\(X_S\)</span>
draw-by-draw, which induces a posterior predictive distribution for the
linear coefficients under the LMM. Again, we use the Mahalanobis loss
for this projection, and summarize the posterior using 95% credible
intervals.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Posterior predictive draws of *all* coefficients:</span></span>
<span><span class="va">post_beta_s</span> <span class="op">=</span> <span class="fu"><a href="../reference/proj_posterior_randint.html">proj_posterior_randint</a></span><span class="op">(</span>post_y_pred <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_y_pred</span>,</span>
<span>                                     XX <span class="op">=</span> <span class="va">X</span>,</span>
<span>                                     sub_x <span class="op">=</span> <span class="va">S_ex</span>,</span>
<span>                                     post_sigma_e <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_sigma_e</span>,</span>
<span>                                     post_sigma_u <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_sigma_u</span>,</span>
<span>                                     post_y_pred_sum <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_y_pred_sum</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">post_beta_s</span><span class="op">)</span> <span class="co"># the coefficients outside S_ex are fixed at zero</span></span>
<span><span class="co">#&gt; [1] 1000   11</span></span>
<span></span>
<span><span class="co"># Compute 95% credible intervals for the nonzero entries:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html" class="external-link">apply</a></span><span class="op">(</span><span class="va">post_beta_s</span><span class="op">[</span>,<span class="va">S_ex</span><span class="op">]</span>, <span class="fl">2</span>, </span>
<span>        <span class="va">quantile</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.05</span><span class="op">/</span><span class="fl">2</span>, <span class="fl">1</span> <span class="op">-</span> <span class="fl">0.05</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;            2.5%      97.5%</span></span>
<span><span class="co">#&gt; [1,] -1.3765692 -1.0718788</span></span>
<span><span class="co">#&gt; [2,]  1.7156488  2.1201515</span></span>
<span><span class="co">#&gt; [3,] -0.7322601 -0.3374807</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="bayesian-subset-search">Bayesian subset search<a class="anchor" aria-label="anchor" href="#bayesian-subset-search"></a>
</h2>
<p>To this point, we have focused on point and interval (linear)
summaries for an arbitrary yet fixed subset <span class="math inline">\(S\)</span>. However, we are often interested in
<em>searching</em> across subsets and measuring the predictive
performances. Here, we use the LMM output to generate a collection of
“candidate subsets” using decision analysis <a href="https://jmlr.org/papers/v23/21-0403.html" class="external-link">(Kowal, 2022a)</a>.</p>
<p>Since we use Mahalanobis loss, we first construct a (vectorized)
<span class="math inline">\(mn\)</span>-dimensional response vector
<span class="math inline">\(y^*\)</span> and <span class="math inline">\(mn \times p\)</span> covariate matrix <span class="math inline">\(X^*\)</span> such that <em>squared error loss</em>
with these quantities is equivalent to Mahalanobis loss.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Access the "X" and "Y" matrices needed for the search:</span></span>
<span><span class="va">objXY</span> <span class="op">=</span> <span class="fu"><a href="../reference/getXY_randint.html">getXY_randint</a></span><span class="op">(</span>XX <span class="op">=</span> <span class="va">X</span>, </span>
<span>                      post_y_pred <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_y_pred</span>,</span>
<span>                      post_sigma_e <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_sigma_e</span>,</span>
<span>                      post_sigma_u <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_sigma_u</span>,</span>
<span>                      post_y_pred_sum <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_y_pred_sum</span><span class="op">)</span></span>
<span><span class="va">X_star</span> <span class="op">=</span> <span class="va">objXY</span><span class="op">$</span><span class="va">X_star</span>; <span class="va">y_star</span> <span class="op">=</span> <span class="va">objXY</span><span class="op">$</span><span class="va">y_star</span>; <span class="fu"><a href="https://rdrr.io/r/base/rm.html" class="external-link">rm</a></span><span class="op">(</span><span class="va">objXY</span><span class="op">)</span></span></code></pre></div>
<p>For small <span class="math inline">\(p\)</span> it may be possible
to enumerate all possible subsets. Here, we screen to the “best”
<code>n_best = 50</code> models of each size according to squared error
loss. We store these in a Boolean matrix <code>indicators</code>: each
row is an individual subset, while the columns indicate which variables
are included (<code>TRUE</code>) or excluded (<code>FALSE</code>).</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">indicators</span> <span class="op">=</span> <span class="fu"><a href="../reference/branch_and_bound.html">branch_and_bound</a></span><span class="op">(</span>yy <span class="op">=</span> <span class="va">y_star</span>, <span class="co"># response is the fitted values</span></span>
<span>                             XX <span class="op">=</span> <span class="va">X_star</span>,            <span class="co"># covariates</span></span>
<span>                             n_best <span class="op">=</span> <span class="fl">50</span>        <span class="co"># restrict to the "best" 50 subsets of each size</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Inspect:</span></span>
<span><span class="va">indicators</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span></span>
<span><span class="co">#&gt;            X1    X2    X3    X4    X5    X6    X7    X8    X9   X10</span></span>
<span><span class="co">#&gt; force_in TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span><span class="co">#&gt;          TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span><span class="co">#&gt;          TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span><span class="co">#&gt;          TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span><span class="co">#&gt;          TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE</span></span>
<span></span>
<span><span class="co"># Dimensions:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">indicators</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 362  11</span></span>
<span></span>
<span><span class="co"># Summarize the model sizes:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html" class="external-link">table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colSums.html" class="external-link">rowSums</a></span><span class="op">(</span><span class="va">indicators</span><span class="op">)</span><span class="op">)</span> <span class="co"># note: intercept always included</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  1  2  3  4  5  6  7  8  9 10 11 </span></span>
<span><span class="co">#&gt;  1 10 45 50 50 50 50 50 45 10  1</span></span></code></pre></div>
<p>When <span class="math inline">\(p \gg 30\)</span>, it is recommended
to use the <code>prescreen</code> function, which restricts the search
to include only <code>num_to_keep</code> possible active variables. This
makes the branch-and-bound algorithm feasible for very large <span class="math inline">\(p\)</span>.</p>
</div>
<div class="section level2">
<h2 id="the-acceptable-family-of-near-optimal-subsets">The acceptable family of “near-optimal” subsets<a class="anchor" aria-label="anchor" href="#the-acceptable-family-of-near-optimal-subsets"></a>
</h2>
<p>From this large collection of 362 candidate subsets, we seek to
filter to the <strong>acceptable family</strong> of subsets, i.e., those
“near-optimal” subsets that predict about as well as the “best” subset.
These are computed based on 10-fold cross-validation, and use the
out-of-sample predictive distribution from the LMM to provide
uncertainty quantification for predictive accuracy.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute the acceptable family:</span></span>
<span><span class="va">accept_info</span> <span class="op">=</span> <span class="fu"><a href="../reference/accept_family_randint.html">accept_family_randint</a></span><span class="op">(</span>post_y_pred <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_y_pred</span>,</span>
<span>                                    post_lpd <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_lpd</span>,</span>
<span>                                    post_sigma_e <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_sigma_e</span>,</span>
<span>                                    post_sigma_u <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_sigma_u</span>,</span>
<span>                                    XX <span class="op">=</span> <span class="va">X</span>, YY <span class="op">=</span> <span class="va">Y</span>,</span>
<span>                                    indicators <span class="op">=</span> <span class="va">indicators</span>,</span>
<span>                                    post_y_pred_sum <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_y_pred_sum</span><span class="op">)</span></span></code></pre></div>
<p><img src="Linear-mixed_files/figure-html/accept-1.png" width="768"></p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># How many subsets are in the acceptable family?</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">accept_info</span><span class="op">$</span><span class="va">all_accept</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 31</span></span>
<span></span>
<span><span class="co"># These are the rows of `indicators` that belong to the acceptable family:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">accept_info</span><span class="op">$</span><span class="va">all_accept</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 157 207 208 209 210 211</span></span>
<span></span>
<span><span class="co"># An example acceptable subset:</span></span>
<span><span class="va">ex_accept</span> <span class="op">=</span> <span class="va">accept_info</span><span class="op">$</span><span class="va">all_accept</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/which.html" class="external-link">which</a></span><span class="op">(</span><span class="va">indicators</span><span class="op">[</span><span class="va">ex_accept</span>,<span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt; X1 X2 X3 X4 X5 X6 </span></span>
<span><span class="co">#&gt;  1  2  3  4  5  6</span></span></code></pre></div>
<p>The plot shows how the out-of-sample predictive performance varies
across subsets of different sizes, specifically relative (% change) to
the “best” subset (by minimum cross-validated error; dashed gray
vertical line). The x-marks are the (usual) empirical cross-validated
error, while the intervals leverage the predictive distribution from the
LMM to quantify uncertainty in the out-of-sample predictive performance.
While performance improves as variables are added, it is clear that
several smaller subsets are highly competitive—especially when
accounting for the predictive uncertainty.</p>
</div>
<div class="section level2">
<h2 id="subset-selection-the-smallest-acceptable-subset">Subset selection: the smallest acceptable subset<a class="anchor" aria-label="anchor" href="#subset-selection-the-smallest-acceptable-subset"></a>
</h2>
<p>If we wish to <strong>select</strong> a single subset, a compelling
representative of the acceptable family is the <strong>smallest</strong>
acceptable subset. This choice favors parsimony, while its membership in
the acceptable family implies that it meets a high standard for
predictive accuracy. From the previous plot, we select the smallest
subset for which the intervals include zero (solid gray vertical
line).</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simplest acceptable subset:</span></span>
<span><span class="va">beta_hat_small</span> <span class="op">=</span> <span class="va">accept_info</span><span class="op">$</span><span class="va">beta_hat_small</span></span>
<span></span>
<span><span class="co"># Which coefficients are nonzero:</span></span>
<span><span class="va">S_small</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html" class="external-link">which</a></span><span class="op">(</span><span class="va">beta_hat_small</span> <span class="op">!=</span> <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># How many coefficients are nonzero:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">S_small</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 6</span></span></code></pre></div>
<p>The “best” subset by minimum cross-validation often includes many
extraneous variables, which is a well-known (and undesirable) byproduct
of cross-validation.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Acceptable subset that minimizes CV error:</span></span>
<span><span class="va">beta_hat_min</span> <span class="op">=</span> <span class="va">accept_info</span><span class="op">$</span><span class="va">beta_hat_min</span></span>
<span></span>
<span><span class="co"># Typically much larger (and often too large...)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">beta_hat_min</span> <span class="op">!=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 8</span></span></code></pre></div>
<p>For reference, the true model size is 6.</p>
<p>Returning to the <em>smallest</em> acceptable subset, we can obtain
posterior samples and credible intervals for the coefficients as
before:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Draws from the posterior predictive distribution</span></span>
<span></span>
<span><span class="co"># Posterior predictive draws of *all* coefficients:</span></span>
<span><span class="va">post_beta_small</span> <span class="op">=</span> <span class="fu"><a href="../reference/proj_posterior_randint.html">proj_posterior_randint</a></span><span class="op">(</span>post_y_pred <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_y_pred</span>,</span>
<span>                                     XX <span class="op">=</span> <span class="va">X</span>,</span>
<span>                                     sub_x <span class="op">=</span> <span class="va">S_small</span>,</span>
<span>                                     post_sigma_e <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_sigma_e</span>,</span>
<span>                                     post_sigma_u <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_sigma_u</span>,</span>
<span>                                     post_y_pred_sum <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">post_y_pred_sum</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute 95% credible intervals for the nonzero entries:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html" class="external-link">apply</a></span><span class="op">(</span><span class="va">post_beta_small</span><span class="op">[</span>,<span class="va">S_small</span><span class="op">]</span>, <span class="fl">2</span>, </span>
<span>        <span class="va">quantile</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.05</span><span class="op">/</span><span class="fl">2</span>, <span class="fl">1</span> <span class="op">-</span> <span class="fl">0.05</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;            2.5%      97.5%</span></span>
<span><span class="co">#&gt; [1,] -1.3264852 -1.0235191</span></span>
<span><span class="co">#&gt; [2,]  0.6362557  1.1966796</span></span>
<span><span class="co">#&gt; [3,]  0.8541809  1.3889162</span></span>
<span><span class="co">#&gt; [4,]  0.6485005  1.1382481</span></span>
<span><span class="co">#&gt; [5,] -1.1156088 -0.7929485</span></span>
<span><span class="co">#&gt; [6,] -1.2317982 -0.8719539</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="variable-importance-from-acceptable-subsets">Variable importance from acceptable subsets<a class="anchor" aria-label="anchor" href="#variable-importance-from-acceptable-subsets"></a>
</h2>
<p>Another useful summary of the acceptable family is the
<strong>variable importance</strong>, which reports, for each variable
<span class="math inline">\(j\)</span>, the proportion of acceptable
subsets in which <span class="math inline">\(j\)</span> appears. We are
particularly interested in distinguishing among those variables that
occur in <em>all</em>, <em>some</em>, or <em>no</em> acceptable subsets,
which provides insight about which variables are indispensable
(“keystone covariates”) and which variables are part of a “predictively
plausible” explanation.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Variable importance: proportion of *acceptable subsets* in which each variable appears</span></span>
<span><span class="va">vi_e</span> <span class="op">=</span> <span class="fu"><a href="../reference/var_imp.html">var_imp</a></span><span class="op">(</span>indicators <span class="op">=</span> <span class="va">indicators</span>,</span>
<span>               all_accept <span class="op">=</span> <span class="va">accept_info</span><span class="op">$</span><span class="va">all_accept</span><span class="op">)</span><span class="op">$</span><span class="va">vi_inc</span></span>
<span></span>
<span><span class="co"># "Keystone covariates" that appear in *all* acceptable families:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/which.html" class="external-link">which</a></span><span class="op">(</span><span class="va">vi_e</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; 1 2 3 4 5 6 </span></span>
<span><span class="co">#&gt; 1 2 3 4 5 6</span></span>
<span></span>
<span><span class="co"># Irrelevant covariates that appear in *no* acceptable families:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/which.html" class="external-link">which</a></span><span class="op">(</span><span class="va">vi_e</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> </span>
<span><span class="co">#&gt; named integer(0)</span></span>
<span></span>
<span><span class="co"># Visualize:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/barplot.html" class="external-link">barplot</a></span><span class="op">(</span><span class="va">vi_e</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/order.html" class="external-link">order</a></span><span class="op">(</span><span class="va">vi_e</span>, <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">ncol</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">:</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">]</span>, <span class="co"># order...</span></span>
<span>        horiz <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>        main <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">'Variable importance for the acceptable family'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html" class="external-link">abline</a></span><span class="op">(</span>v <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p><img src="Linear-mixed_files/figure-html/vi-1.png" width="768"></p>
<p>Note that the covariates are highly correlated in this simulated
example (and <span class="math inline">\(p\)</span> is moderate), so it
is reasonable to expect that many covariates are roughly interchangeable
in terms of predictive accuracy.</p>
</div>
<div class="section level2">
<h2 id="comparing-with-traditional-posterior-summaries">Comparing with traditional posterior summaries<a class="anchor" aria-label="anchor" href="#comparing-with-traditional-posterior-summaries"></a>
</h2>
<p>Typically, Bayesian linear regression would report the posterior
expectations and 95% posterior credible intervals of the regression
coefficients <span class="math inline">\(\beta\)</span>. We plot these
together with the point and interval estimates for the <em>smallest</em>
acceptable model:</p>
<p><img src="Linear-mixed_files/figure-html/plot-1.png" width="768"></p>
<p>The traditional model summaries are completely dense: the point
estimates <span class="math inline">\(\hat \beta\)</span> are nonzero
for <em>all</em> covariates. By comparison, the point estimates from the
smallest acceptable subset are sparse, with only 6 active coefficients.
By design, the smallest acceptable subset only reports interval
estimates for these active coefficients. In this example, the intervals
are narrower for the smallest acceptable subset, while the traditional
intervals produce large intervals even for the truly zero
coefficients.</p>
</div>
<div class="section level2">
<h2 id="conclusion">Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"></a>
</h2>
<p>We have sought to demonstrate that <code>BayesSubsets</code> is a
useful accompaniment to Bayesian LMM regression workflow. Given a
Bayesian LMM—and specifically, a random intercept model for longitudinal
data regression—we have shown how to compute</p>
<ol style="list-style-type: decimal">
<li><p>Optimal linear summaries for any subset of covariates;</p></li>
<li><p>Accompanying uncertainty quantification via posterior
(predictive) distributions and intervals;</p></li>
<li><p>Bayesian subset search using decision analysis;</p></li>
<li><p>The acceptable family of near-optimal subsets; and</p></li>
<li><p>Key summaries of the acceptable family, including the smallest
acceptable subset and a variable importance metric.</p></li>
</ol>
<p>Each of these quantities incorporates the structured dependence
(here, due to repeated measurements) via Mahalanobis loss.</p>
<p>When a single subset is required and parsimony is valued, then we
recommend the <strong>smallest acceptable subset</strong>. However, we
caution against the overreliance on any single subset without compelling
motivation. A key contribution of the acceptable family is that it
identifies <em>many</em> competing explanations (subsets) that are
nearly indistinguishable in predictive accuracy. From a purely
predictive perspective, we cannot completely rule out any member of the
acceptable family. Thus, we further recommend reporting the
<strong>variable importance</strong> as a default, variable-specific
summary of the acceptable family.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Dan Kowal.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
